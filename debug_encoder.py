"""
debug_encoder.py - ç¼–ç å™¨è°ƒè¯•å’ŒéªŒè¯è„šæœ¬
=====================================

åŠŸèƒ½æè¿°:
- è®­ç»ƒå›¾å˜åˆ†è‡ªç¼–ç å™¨(GraphVAE)æ¨¡å‹50è½®
- åœ¨å…ƒå­¦ä¹ (meta-learning)æµ‹è¯•é›†ä¸Šè¯„ä¼°æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†çš„å‡†ç¡®ç‡
- è¯Šæ–­ç‰¹å¾è´¨é‡ï¼Œåˆ†ææ¨¡å‹æ€§èƒ½ç“¶é¢ˆ

é‡è¦æ›´æ–°:
- ç°åœ¨ä½¿ç”¨vae_trainer._extract_featuresè¿›è¡Œç»Ÿä¸€çš„ç‰¹å¾æå–
- ç‰¹å¾ç»´åº¦å˜æ›´: [batch_size, latent_dim] -> [batch_size, latent_dim*2] (mean+maxç»„åˆæ± åŒ–)
- æµå½¢å‡ ä½•å¤„ç†: æ­£ç¡®æ”¯æŒåŒæ›²ç©ºé—´å’Œæ¬§å‡ é‡Œå¾—ç©ºé—´
- ä¸GraphVAEã€Classifierç­‰ç»„ä»¶å®Œå…¨ä¸€è‡´çš„ç‰¹å¾è¡¨å¾

ä½¿ç”¨åœºæ™¯:
- è°ƒè¯•è®­ç»ƒå¥½çš„VAEç¼–ç å™¨æ€§èƒ½
- éªŒè¯å°‘æ ·æœ¬å­¦ä¹ (Few-shot Learning)ä»»åŠ¡çš„æ•ˆæœ
- åˆ†æç‰¹å¾è´¨é‡å’Œå¯åˆ†æ€§
"""

# æ ‡å‡†åº“å¯¼å…¥
import os
from datetime import datetime

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from omegaconf import OmegaConf
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from vae_trainer import train_vae, meta_eval, _extract_features, _test_with_prototypical_networks
from utils.config_utils import load_config
from utils.graph_utils import node_flags
from utils.manifolds_utils import get_manifold


def evaluate_task_with_prototypical_networks(encoder, task, config, device):
    """
    ä½¿ç”¨åŸå‹ç½‘ç»œæ–¹æ³•åœ¨å•ä¸ªå°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šè¯„ä¼°ç¼–ç å™¨æ€§èƒ½

    åŸå‹ç½‘ç»œ(Prototypical Networks)æ˜¯ä¸€ç§ç»å…¸çš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—æ”¯æŒé›†æ ·æœ¬çš„ç±»åˆ«åŸå‹ï¼Œ
    ç„¶ååŸºäºæŸ¥è¯¢é›†æ ·æœ¬ä¸åŸå‹çš„è·ç¦»è¿›è¡Œåˆ†ç±»ã€‚

    å‚æ•°:
        encoder: è®­ç»ƒå¥½çš„å›¾ç¼–ç å™¨ï¼Œç”¨äºæå–å›¾ç‰¹å¾
        task: å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼ŒåŒ…å«æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†
        config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«N-wayç­‰å‚æ•°
        device: è®¡ç®—è®¾å¤‡(CPU/GPU)

    è¿”å›:
        tuple: (æ”¯æŒé›†å‡†ç¡®ç‡, æŸ¥è¯¢é›†å‡†ç¡®ç‡)
    """
    # è·å–N-wayè®¾ç½®(å³åˆ†ç±»ç±»åˆ«æ•°)
    N_way = config.fsl_task.N_way

    # === æ•°æ®é¢„å¤„ç†ï¼šæå–ä»»åŠ¡ä¸­çš„æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†æ•°æ® ===
    # æ”¯æŒé›†ï¼šç”¨äºæ„å»ºç±»åˆ«åŸå‹çš„å¸¦æ ‡ç­¾æ ·æœ¬
    support_x = task["support_set"]["x"].to(device)  # èŠ‚ç‚¹ç‰¹å¾ [batch_size, num_nodes, feat_dim]
    support_adj = task["support_set"]["adj"].to(
        device
    )  # é‚»æ¥çŸ©é˜µ [batch_size, num_nodes, num_nodes]
    support_labels = task["support_set"]["label"].to(device)  # å›¾çº§åˆ«æ ‡ç­¾ [batch_size]

    # æŸ¥è¯¢é›†ï¼šéœ€è¦è¿›è¡Œåˆ†ç±»é¢„æµ‹çš„æ ·æœ¬
    query_x = task["query_set"]["x"].to(device)
    query_adj = task["query_set"]["adj"].to(device)
    query_labels = task["query_set"]["label"].to(device)

    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œä¾¿äºè¿½è¸ªæ•°æ®ç»´åº¦
    print(f"  Support set shape: {support_x.shape}, labels: {support_labels}")
    print(f"  Query set shape: {query_x.shape}, labels: {query_labels}")
    print(f"  N_way: {N_way}")

    # === ç‰¹å¾æå–ï¼šä½¿ç”¨ç¼–ç å™¨æå–å›¾çš„æ½œåœ¨è¡¨å¾ ===
    encoder.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå…³é—­dropoutç­‰
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—
        # æ³¨æ„ï¼š_extract_featuresç°åœ¨è¿”å›ç»„åˆç‰¹å¾ [batch_size, latent_dim*2] (mean+maxæ± åŒ–)
        # è¿™äº›ç‰¹å¾å·²ç»åœ¨åˆ‡ç©ºé—´ä¸­ï¼Œåº”è¯¥ä½¿ç”¨æ¬§å‡ é‡Œå¾—å‡ ä½•å¤„ç†
        support_features = _extract_features(encoder, support_x, support_adj, device)
        query_features = _extract_features(encoder, query_x, query_adj, device)

    print(f"  Support features shape: {support_features.shape} (mean+maxç»„åˆç‰¹å¾)")
    print(f"  Query features shape: {query_features.shape} (mean+maxç»„åˆç‰¹å¾)")

    # === ç‰¹å¾è´¨é‡è¯Šæ–­ï¼šåˆ†ææå–ç‰¹å¾çš„ç»Ÿè®¡ç‰¹æ€§å’Œå¯åˆ†æ€§ ===
    _diagnose_feature_quality(support_features, support_labels, query_features, query_labels, N_way)

    # === åŸå‹ç½‘ç»œè¯„ä¼°ï¼šåˆ†åˆ«è®¡ç®—æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†çš„åˆ†ç±»å‡†ç¡®ç‡ ===
    # å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸå‹ç½‘ç»œï¼Œå› ä¸ºç‰¹å¾å·²ç»åœ¨åˆ‡ç©ºé—´ä¸­
    print("  è¯„ä¼°æ”¯æŒé›†ï¼ˆä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸå‹ç½‘ç»œï¼‰:")
    # æ”¯æŒé›†ä¸Šçš„è‡ªæˆ‘éªŒè¯(ç”¨æ”¯æŒé›†æ—¢åšåŸå‹åˆåšæµ‹è¯•)
    acc_support = _test_with_prototypical_networks(
        support_features, support_labels, support_features, support_labels, N_way, device
    )

    # æŸ¥è¯¢é›†åˆ†ç±»(ç”¨æ”¯æŒé›†ç‰¹å¾å’Œæ ‡ç­¾åšåŸå‹ï¼ŒæŸ¥è¯¢é›†åšåˆ†ç±»)
    print("  è¯„ä¼°æŸ¥è¯¢é›†ï¼ˆä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸå‹ç½‘ç»œï¼‰:")
    acc_query = _test_with_prototypical_networks(
        support_features, support_labels, query_features, query_labels, N_way, device
    )
    return acc_support, acc_query


def _diagnose_feature_quality(
    support_features, support_labels, query_features, query_labels, n_way
):
    """
    ç‰¹å¾è´¨é‡è¯Šæ–­å‡½æ•° - å…¨é¢åˆ†ææå–ç‰¹å¾çš„ç»Ÿè®¡ç‰¹æ€§å’Œå¯åˆ†æ€§

    è¯¥å‡½æ•°é€šè¿‡å¤šä¸ªç»´åº¦åˆ†æç‰¹å¾è´¨é‡:
    1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯(å‡å€¼ã€æ–¹å·®ã€èŒƒæ•°ç­‰)
    2. ç±»åˆ«åŸå‹åˆ†æ(ç±»å†…èšé›†æ€§)
    3. ç±»é—´è·ç¦»åˆ†æ(ç±»é—´å¯åˆ†æ€§)
    4. ç‰¹å¾æœ‰æ•ˆæ€§æ£€æŸ¥(æ•°å€¼ç¨³å®šæ€§)
    5. æ”¹è¿›å»ºè®®

    å‚æ•°:
        support_features: æ”¯æŒé›†ç‰¹å¾ [num_support, feature_dim]
        support_labels: æ”¯æŒé›†æ ‡ç­¾ [num_support]
        query_features: æŸ¥è¯¢é›†ç‰¹å¾ [num_query, feature_dim]
        query_labels: æŸ¥è¯¢é›†æ ‡ç­¾ [num_query]
        n_way: åˆ†ç±»ç±»åˆ«æ•°
    """
    print(f"\n  === ç‰¹å¾è´¨é‡è¯Šæ–­ ===")

    # === 1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯åˆ†æ ===
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾ç”¨äºæ•´ä½“ç»Ÿè®¡
    all_features = torch.cat([support_features, query_features], dim=0)
    feature_mean = all_features.mean(dim=0)  # æ¯ä¸ªç»´åº¦çš„å‡å€¼
    feature_std = all_features.std(dim=0)  # æ¯ä¸ªç»´åº¦çš„æ ‡å‡†å·®

    print(f"  ç‰¹å¾ç»´åº¦: {all_features.shape[1]}")
    print(f"  ç‰¹å¾å‡å€¼èŒƒå›´: [{feature_mean.min():.4f}, {feature_mean.max():.4f}]")
    print(f"  ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: [{feature_std.min():.4f}, {feature_std.max():.4f}]")
    print(
        f"  ç‰¹å¾èŒƒæ•°: å‡å€¼={torch.norm(all_features, dim=1).mean():.4f}, "
        f"æ ‡å‡†å·®={torch.norm(all_features, dim=1).std():.4f}"
    )

    # === 2. ç±»åˆ«åŸå‹åˆ†æ - è¯„ä¼°ç±»å†…èšé›†æ€§ ===
    print(f"  å„ç±»åˆ«åŸå‹åˆ†æ:")
    class_prototypes = []
    for class_id in range(n_way):
        # æ‰¾åˆ°å½“å‰ç±»åˆ«çš„æ‰€æœ‰æ”¯æŒé›†æ ·æœ¬
        class_mask = support_labels == class_id
        if class_mask.sum() > 0:
            # è®¡ç®—è¯¥ç±»åˆ«çš„ç‰¹å¾å‡å€¼ä½œä¸ºåŸå‹
            class_features = support_features[class_mask]
            class_proto = class_features.mean(dim=0)
            class_prototypes.append(class_proto)

            # è®¡ç®—ç±»å†…æ–¹å·®(è¡¡é‡ç±»å†…èšé›†æ€§)
            intra_variance = ((class_features - class_proto.unsqueeze(0)) ** 2).mean().item()
            print(f"    ç±»åˆ«{class_id}: æ ·æœ¬æ•°={class_mask.sum()}, ç±»å†…æ–¹å·®={intra_variance:.4f}")

    # === 3. ç±»é—´è·ç¦»åˆ†æ - è¯„ä¼°ç±»é—´å¯åˆ†æ€§ ===
    if len(class_prototypes) >= 2:
        class_prototypes = torch.stack(class_prototypes)
        inter_distances = []

        # è®¡ç®—æ‰€æœ‰ç±»åˆ«åŸå‹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»
        for i in range(len(class_prototypes)):
            for j in range(i + 1, len(class_prototypes)):
                dist = torch.norm(class_prototypes[i] - class_prototypes[j], p=2).item()
                inter_distances.append(dist)

        print(
            f"  ç±»é—´è·ç¦»: æœ€å°={min(inter_distances):.4f}, "
            f"æœ€å¤§={max(inter_distances):.4f}, "
            f"å¹³å‡={sum(inter_distances)/len(inter_distances):.4f}"
        )

        # === å¯åˆ†æ€§è¯„ä¼°ï¼šåŸºäºç±»é—´è·ç¦»åˆ¤æ–­åˆ†ç±»éš¾åº¦ ===
        avg_inter_dist = sum(inter_distances) / len(inter_distances)
        if avg_inter_dist < 1.0:
            print(f"  âŒ ç±»é—´è·ç¦»è¿‡å° (<1.0), å»ºè®®å¢åŠ åŸå‹åˆ†ç¦»æŸå¤±")
        elif avg_inter_dist > 5.0:
            print(f"  âœ… ç±»é—´è·ç¦»è‰¯å¥½ (>5.0)")
        else:
            print(f"  âš ï¸  ç±»é—´è·ç¦»ä¸­ç­‰ (1.0-5.0), å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–")

    # === 4. ç‰¹å¾æœ‰æ•ˆæ€§æ£€æŸ¥ - æ•°å€¼ç¨³å®šæ€§éªŒè¯ ===
    # æ£€æŸ¥é›¶ç‰¹å¾å‘é‡(å¯èƒ½è¡¨ç¤ºç¼–ç å™¨è¾“å‡ºé€€åŒ–)
    zero_features = (all_features.abs() < 1e-6).all(dim=1).sum().item()
    if zero_features > 0:
        print(f"  âŒ å‘ç°{zero_features}ä¸ªé›¶ç‰¹å¾å‘é‡")

    # æ£€æŸ¥NaNå€¼(å¯èƒ½è¡¨ç¤ºæ•°å€¼è®¡ç®—ä¸ç¨³å®š)
    nan_features = torch.isnan(all_features).any(dim=1).sum().item()
    if nan_features > 0:
        print(f"  âŒ å‘ç°{nan_features}ä¸ªåŒ…å«NaNçš„ç‰¹å¾")

    # æ£€æŸ¥æ— ç©·å€¼(å¯èƒ½è¡¨ç¤ºæ¢¯åº¦çˆ†ç‚¸æˆ–æ•°å€¼æº¢å‡º)
    inf_features = torch.isinf(all_features).any(dim=1).sum().item()
    if inf_features > 0:
        print(f"  âŒ å‘ç°{inf_features}ä¸ªåŒ…å«Infçš„ç‰¹å¾")

    # å¦‚æœç‰¹å¾æ•°å€¼å¥åº·ï¼Œç»™å‡ºæ­£é¢åé¦ˆ
    if zero_features == 0 and nan_features == 0 and inf_features == 0:
        print(f"  âœ… ç‰¹å¾æ•°å€¼å¥åº·")

    # === 5. æ”¹è¿›å»ºè®® - åŸºäºè¯Šæ–­ç»“æœæä¾›ä¼˜åŒ–æ–¹å‘ ===
    # åªæœ‰å½“ç±»é—´è·ç¦»è¿‡å°æ—¶æ‰æä¾›å»ºè®®(é¿å…å±€éƒ¨å˜é‡æœªå®šä¹‰é”™è¯¯)
    if "avg_inter_dist" in locals() and avg_inter_dist < 1.0:
        print(f"\n  ğŸ’¡ æ”¹è¿›å»ºè®®:")
        print(f"    1. å¢åŠ åŸå‹åˆ†ç¦»æŸå¤±æƒé‡ (sep_proto_weight)")
        print(f"    2. å¢åŠ è®­ç»ƒè½®æ•° (å½“å‰50è½®å¯èƒ½ä¸å¤Ÿ)")
        print(f"    3. è°ƒæ•´å­¦ä¹ ç‡æˆ–ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦")
        print(f"    4. æ£€æŸ¥VAEæŸå¤±æƒé‡é…ç½®")


def _test_with_hyperbolic_prototypical_networks(
    support_features, support_labels, query_features, query_labels, n_way, device, manifold
):
    """
    ä½¿ç”¨åŒæ›²å‡ ä½•çš„åŸå‹ç½‘ç»œè¿›è¡Œåˆ†ç±» - æ­£ç¡®å¤„ç†PoincarÃ©çƒæµå½¢
    """
    try:
        print(
            f"    åŒæ›²åŸå‹ç½‘ç»œè¾“å…¥: support_features={support_features.shape}, support_labels={support_labels}"
        )
        print(f"    query_features={query_features.shape}, query_labels={query_labels}")
        print(f"    æµå½¢ç±»å‹: {manifold.name}, æ›²ç‡: {manifold.c}")

        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦åœ¨æµå½¢ä¸Š
        print(f"    æ”¯æŒé›†ç‰¹å¾èŒƒæ•°: {torch.norm(support_features, dim=1).max().item():.6f}")
        print(f"    æŸ¥è¯¢é›†ç‰¹å¾èŒƒæ•°: {torch.norm(query_features, dim=1).max().item():.6f}")

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒæ›²åŸå‹
        support_protos = []
        for class_id in range(n_way):
            # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ”¯æŒé›†æ ·æœ¬
            class_mask = support_labels == class_id
            if class_mask.sum() > 0:
                class_features = support_features[class_mask]
                # ä½¿ç”¨åŒæ›²å‡ ä½•çš„åŠ æƒä¸­ç‚¹è®¡ç®—åŸå‹
                if len(class_features) == 1:
                    class_proto = class_features[0]
                else:
                    # åœ¨åŒæ›²ç©ºé—´ä¸­è®¡ç®—ç­‰æƒé‡ä¸­ç‚¹
                    weights = torch.ones(len(class_features), device=device) / len(class_features)
                    class_proto = manifold.weighted_midpoint(class_features, weights)
                support_protos.append(class_proto)

                # è®¡ç®—ç±»å†…åŒæ›²è·ç¦»çš„ç»Ÿè®¡ä¿¡æ¯
                if class_features.shape[0] > 1:
                    intra_distances = []
                    for i in range(len(class_features)):
                        for j in range(i + 1, len(class_features)):
                            dist = manifold.dist(class_features[i], class_features[j])
                            intra_distances.append(dist.item())
                    avg_intra_dist = (
                        sum(intra_distances) / len(intra_distances) if intra_distances else 0
                    )
                    print(f"    ç±»åˆ«{class_id}å†…éƒ¨å¹³å‡åŒæ›²è·ç¦»: {avg_intra_dist:.4f}")
            else:
                print(f"    è­¦å‘Š: ç±»åˆ«{class_id}æ²¡æœ‰æ”¯æŒé›†æ ·æœ¬")
                # ä½¿ç”¨æµå½¢çš„åŸç‚¹ä½œä¸ºé»˜è®¤åŸå‹
                support_protos.append(manifold.origin(support_features.shape[-1:], device=device))

        support_protos = torch.stack(support_protos)  # [n_way, feature_dim]
        print(f"    åŸå‹å½¢çŠ¶: {support_protos.shape}")

        # è®¡ç®—åŸå‹ä¹‹é—´çš„åŒæ›²è·ç¦»
        print(f"    åŸå‹é—´åŒæ›²è·ç¦»çŸ©é˜µ:")
        for i in range(n_way):
            for j in range(n_way):
                if i != j:
                    dist = manifold.dist(support_protos[i], support_protos[j])
                    print(f"      ç±»åˆ«{i}ä¸ç±»åˆ«{j}åŒæ›²è·ç¦»: {dist.item():.4f}")

        # è®¡ç®—æŸ¥è¯¢æ ·æœ¬ä¸åŸå‹ä¹‹é—´çš„åŒæ›²è·ç¦»
        distances = []
        for i in range(len(query_features)):
            query_dists = []
            for j in range(len(support_protos)):
                dist = manifold.dist(query_features[i], support_protos[j])
                query_dists.append(dist)
            distances.append(torch.stack(query_dists))
        distances = torch.stack(distances)  # [query_size, n_way]

        # ä½¿ç”¨è·ç¦»çš„è´Ÿæ•°ä½œä¸ºåˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
        scores = -distances

        # é¢„æµ‹
        y_preds = torch.argmax(scores, dim=1)
        print(f"    é¢„æµ‹ç»“æœ: {y_preds}")
        print(f"    çœŸå®æ ‡ç­¾: {query_labels}")

        # è®¡ç®—å‡†ç¡®ç‡
        correct = (y_preds == query_labels).float().sum().item()
        accuracy = correct / len(query_labels) if len(query_labels) > 0 else 0.0

        print(f"    å‡†ç¡®ç‡: {correct}/{len(query_labels)} = {accuracy:.4f}")
        return accuracy

    except Exception as e:
        print(f"    åŒæ›²åŸå‹ç½‘ç»œé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return 0.0


def main():
    # åŠ è½½é…ç½®
    config_path = "configs/letter_high_optimized.yaml"  # å¯æ ¹æ®éœ€è¦æ›´æ¢

    # ä½¿ç”¨OmegaConfç›´æ¥åŠ è½½é…ç½®æ–‡ä»¶
    config = OmegaConf.load(config_path)

    # å¤„ç†æ—¶é—´æˆ³ç­‰åŠ¨æ€å˜é‡
    from datetime import datetime

    if config.timestamp == "auto":
        config.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if config.run_name == "letter_high_poincare_auto":
        config.run_name = f"{config.exp_name}_{config.timestamp}"

    config.vae.train.num_epochs = 500  # è®­ç»ƒ50è½®
    config.vae.train.test_interval = 999  # è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„å€¼ï¼Œé¿å…è®­ç»ƒæ—¶è¯„ä¼°

    # è®­ç»ƒVAE
    result = train_vae(config)
    print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {result['best_checkpoint']}")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint = torch.load(result["best_checkpoint"], map_location="cpu", weights_only=False)

    # è·å–è®¾å¤‡ä¿¡æ¯
    from utils.loader import load_device

    device = load_device(config)

    # ç›´æ¥ä»checkpointä¸­é‡å»ºæ¨¡å‹
    from types import SimpleNamespace
    from models.GraphVAE import GraphVAE

    # é‡å»ºVAEé…ç½®
    saved_config = checkpoint["model_config"]
    vae_config = SimpleNamespace()
    vae_config.pred_node_class = saved_config["vae"]["loss"]["pred_node_class"]
    vae_config.pred_edge = saved_config["vae"]["loss"]["pred_edge"]
    vae_config.use_kl_loss = saved_config["vae"]["loss"]["use_kl_loss"]
    vae_config.use_base_proto_loss = saved_config["vae"]["loss"]["use_base_proto_loss"]
    vae_config.use_sep_proto_loss = saved_config["vae"]["loss"]["use_sep_proto_loss"]

    # é‡å»ºencoder_configï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
    encoder_config_dict = saved_config["vae"]["encoder"]
    encoder_config_dict["input_feature_dim"] = saved_config["data"]["max_feat_num"]
    vae_config.encoder_config = SimpleNamespace(**encoder_config_dict)

    # é‡å»ºdecoder_config
    decoder_config_dict = saved_config["vae"]["decoder"]
    decoder_config_dict["latent_feature_dim"] = saved_config["vae"]["encoder"]["latent_feature_dim"]
    decoder_config_dict["output_feature_dim"] = saved_config["data"]["max_feat_num"]
    vae_config.decoder_config = SimpleNamespace(**decoder_config_dict)

    vae_config.latent_dim = saved_config["vae"]["encoder"]["latent_feature_dim"]
    vae_config.device = device

    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
    model = GraphVAE(vae_config).to(device)
    model.load_state_dict(checkpoint["model_state_dict"])
    encoder = model.encoder

    # Meta-eval: æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†acc
    from utils.data_utils import MyDataset

    dataset = MyDataset(config.data, config.fsl_task)

    print("\n[Meta-eval] è®­ç»ƒé›†ä»»åŠ¡å¾®è°ƒåè¯„ä¼°...")

    # ä½¿ç”¨è®­ç»ƒé›†ä»»åŠ¡è¿›è¡Œå¾®è°ƒå’Œè¯„ä¼°
    N_way = config.fsl_task.N_way
    K_shot = config.fsl_task.K_shot
    R_query = config.fsl_task.R_query

    # é‡‡æ ·ä¸€ä¸ªè®­ç»ƒé›†ä»»åŠ¡
    task = dataset.sample_one_task(
        is_train=True,
        N_way=N_way,
        K_shot=K_shot,
        R_query=R_query,
    )

    if task is not None:
        # ç”¨åŸå‹ç½‘ç»œæ–¹æ³•åˆ†åˆ«è¯„ä¼°æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†
        support_acc, query_acc = evaluate_task_with_prototypical_networks(
            encoder, task, config, device
        )
        print(f"è®­ç»ƒé›†ä»»åŠ¡åŸå‹ç½‘ç»œè¯„ä¼°:")
        print(f"  æ”¯æŒé›†å‡†ç¡®ç‡: {support_acc:.4f}")
        print(f"  æŸ¥è¯¢é›†å‡†ç¡®ç‡: {query_acc:.4f}")
    else:
        print("æ— æ³•é‡‡æ ·åˆ°è®­ç»ƒé›†ä»»åŠ¡")


if __name__ == "__main__":
    main()

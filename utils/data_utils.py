# --- é«˜æ•ˆæ•°æ®åŠ è½½ä¸é‡‡æ ·ï¼ˆå‚è€ƒSMARTçš„æ ‡ç­¾å¤„ç†ç­–ç•¥ï¼‰ ---
import os
import pathlib
import pickle
import networkx as nx
import torch
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from utils.graph_utils import graphs_to_tensor
import json
from collections import defaultdict
from numpy.random import RandomState
import multiprocessing as mp
from functools import partial
import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import warnings


# --- ä¼˜åŒ–åçš„æ•°æ®æ–‡ä»¶è¯»å–ä¸é¢„å¤„ç† ---
def _process_single_graph(
    graph_data, use_degree_as_tag, original_node_tag_map, tag_to_idx_map, feature_dimension
):
    """å¤„ç†å•ä¸ªå›¾çš„è¾…åŠ©å‡½æ•°ï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†"""
    graph_item = graph_data
    nx_g = graph_item["nx_graph"]
    nx_g.graph["label"] = graph_item["label"]

    if use_degree_as_tag:
        node_tags_for_features = [d for _, d in nx_g.degree()]
    else:
        node_tags_for_features = graph_item["raw_tags"]

    for node_id, node_obj in nx_g.nodes(data=True):
        tag_val = node_tags_for_features[node_id]
        one_hot_vec = np.zeros(feature_dimension, dtype=np.float32)
        if feature_dimension > 0 and tag_val in tag_to_idx_map:
            one_hot_vec[tag_to_idx_map[tag_val]] = 1.0
        elif feature_dimension == 1 and not tag_to_idx_map:
            one_hot_vec[0] = 0.0
        nx_g.nodes[node_id]["feature"] = one_hot_vec

    return nx_g


def load_from_file_optimized(data_config, use_degree_as_tag):
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œå‚è€ƒSMARTæ¡†æ¶ä¿ç•™åŸå§‹æ ‡ç­¾
    """
    dataset_name = data_config.name
    base_dir = pathlib.Path("./datasets") / dataset_name
    file_path = base_dir / f"{dataset_name}.txt"
    save_file_path = base_dir / f"{dataset_name}_processed.pkl"  # ä½¿ç”¨ä¸åŸç‰ˆç›¸åŒçš„æ–‡ä»¶å
    force_reload = getattr(data_config, "force_reload_data", False)

    if not force_reload and save_file_path.exists():
        print(f"Loading optimized processed data from: {save_file_path}")
        with open(save_file_path, "rb") as f:
            data = pickle.load(f)
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼Œå¦‚æœæ˜¯åŸç‰ˆæ ¼å¼ï¼Œè½¬æ¢ä¸ºä¼˜åŒ–ç‰ˆæ ¼å¼
        if "all_nx_graphs" in data:
            # åŸç‰ˆæ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºGraphDataæ ¼å¼
            all_nx_graphs = data["all_nx_graphs"]
            all_graph_data = []
            for nx_g in all_nx_graphs:
                original_label = nx_g.graph["label"]
                graph_data = GraphData(nx_g, original_label)
                all_graph_data.append(graph_data)
            return all_graph_data, data["tagset"], data["max_node_num"], data["max_feat_dim"]
        else:
            # ä¼˜åŒ–ç‰ˆæ ¼å¼
            return (
                data["all_graph_data"],
                data["tagset"],
                data["max_node_num"],
                data["max_feat_dim"],
            )

    print(
        f"Loading and processing data from: {file_path} (optimized version with label preservation)"
    )

    # Phase 1: Fast file reading - ä¿ç•™åŸå§‹æ ‡ç­¾
    temp_graph_data_list = []
    original_node_tag_map = {}
    max_nodes_observed = 0
    tags_for_onehot_basis = set()

    start_time = time.time()
    with open(file_path, "r") as f:
        lines = f.readlines()

    num_graphs_in_file = int(lines[0].strip())
    temp_graph_data_list = [None] * num_graphs_in_file

    line_idx = 1
    for graph_idx in range(num_graphs_in_file):
        meta_line = lines[line_idx].strip().split()
        num_nodes, original_g_label = int(meta_line[0]), int(meta_line[1])  # ä¿ç•™åŸå§‹æ ‡ç­¾
        line_idx += 1

        edges = []
        current_g_raw_tags = []
        max_nodes_observed = max(max_nodes_observed, num_nodes)

        for node_i in range(num_nodes):
            node_line_parts = lines[line_idx].strip().split()
            line_idx += 1

            original_node_tag = int(node_line_parts[0])
            if original_node_tag not in original_node_tag_map:
                original_node_tag_map[original_node_tag] = len(original_node_tag_map)
            mapped_tag = original_node_tag_map[original_node_tag]
            current_g_raw_tags.append(mapped_tag)

            if not use_degree_as_tag:
                tags_for_onehot_basis.add(mapped_tag)

            num_neighbors = int(node_line_parts[1])
            for k in range(num_neighbors):
                neighbor_id = int(node_line_parts[2 + k])
                if node_i < neighbor_id:
                    edges.append((node_i, neighbor_id))

        # å…³é”®ï¼šä¿å­˜åŸå§‹æ ‡ç­¾ï¼Œä¸è¿›è¡Œä»»ä½•æ˜ å°„
        temp_graph_data_list[graph_idx] = {
            "edges": edges,
            "num_nodes": num_nodes,
            "original_label": original_g_label,  # ä¿ç•™åŸå§‹æ ‡ç­¾
            "raw_tags": current_g_raw_tags,
        }

    print(f"Phase 1 (Graph loading with label preservation): {time.time() - start_time:.2f}s")

    # Phase 2: Degree calculation if needed
    if use_degree_as_tag:
        start_time = time.time()
        for graph_item in temp_graph_data_list:
            degree_count = [0] * graph_item["num_nodes"]
            for i, j in graph_item["edges"]:
                degree_count[i] += 1
                degree_count[j] += 1
            graph_item["raw_tags"] = degree_count
            tags_for_onehot_basis.update(degree_count)
        print(f"Phase 2 (Degree calculation): {time.time() - start_time:.2f}s")

    # Phase 3: Feature encoding
    final_tagset = sorted(list(tags_for_onehot_basis))
    tag_to_idx_map = {tag: i for i, tag in enumerate(final_tagset)}
    feature_dimension = len(final_tagset)
    if feature_dimension == 0 and num_graphs_in_file > 0:
        print("Warning: Feature dimension is 0. Defaulting to a 1-dim zero feature.")
        feature_dimension = 1

    # Phase 4: Build GraphData objects with NetworkX graphs
    start_time = time.time()

    def build_graph_data(graph_data_item):
        """æ„å»ºGraphDataå¯¹è±¡ï¼Œä¿ç•™åŸå§‹æ ‡ç­¾"""
        edges = graph_data_item["edges"]
        num_nodes = graph_data_item["num_nodes"]
        raw_tags = graph_data_item["raw_tags"]
        original_label = graph_data_item["original_label"]  # ä½¿ç”¨åŸå§‹æ ‡ç­¾

        # æ„å»ºNetworkXå›¾
        nx_g = nx.Graph()
        nx_g.add_nodes_from(range(num_nodes))
        nx_g.add_edges_from(edges)

        # æ·»åŠ èŠ‚ç‚¹ç‰¹å¾
        for node_id in range(num_nodes):
            tag_val = raw_tags[node_id]
            one_hot_vec = np.zeros(feature_dimension, dtype=np.float32)
            if feature_dimension > 0 and tag_val in tag_to_idx_map:
                one_hot_vec[tag_to_idx_map[tag_val]] = 1.0
            elif feature_dimension == 1 and not tag_to_idx_map:
                one_hot_vec[0] = 0.0
            nx_g.nodes[node_id]["feature"] = one_hot_vec

        # åˆ›å»ºGraphDataå¯¹è±¡ï¼Œä¿ç•™åŸå§‹æ ‡ç­¾
        return GraphData(nx_g, original_label)

    max_workers = min(mp.cpu_count(), 8)
    if len(temp_graph_data_list) > 100 and max_workers > 1:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            processed_graph_data_list = list(executor.map(build_graph_data, temp_graph_data_list))
    else:
        processed_graph_data_list = [build_graph_data(g) for g in temp_graph_data_list]

    print(
        f"Phase 4 (GraphData construction with label preservation): {time.time() - start_time:.2f}s"
    )

    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    data_to_save = {
        "all_graph_data": processed_graph_data_list,
        "tagset": final_tagset,
        "max_node_num": max_nodes_observed,
        "max_feat_dim": feature_dimension,
    }
    with open(save_file_path, "wb") as f:
        pickle.dump(data_to_save, f)

    return processed_graph_data_list, final_tagset, max_nodes_observed, feature_dimension


class MyDatasetOptimized:
    """
    ä¼˜åŒ–ç‰ˆæ•°æ®é›†ç±»ï¼Œä¸“ä¸ºLetter_highå’ŒFew-shot Learningè®¾è®¡
    åŒ…å«åŸå§‹æ ‡ç­¾ä¿ç•™å’Œé«˜æ•ˆFSLä»»åŠ¡é‡‡æ ·
    """

    def __init__(self, data_config, fsl_task_config=None):
        self.config = data_config
        self.fsl_config = fsl_task_config
        self.dataset_name = data_config.name

        print(f"Initializing MyDataset for {self.dataset_name}...")

        # æ•°æ®åŠ è½½
        self._load_data()

        # è½¬æ¢ä¸ºå¼ é‡æ ¼å¼
        self._convert_to_tensors()

        # FSLç´¢å¼•æ„å»º
        if fsl_task_config:
            self._build_fsl_indices()

        self._print_dataset_summary()

    def _load_data(self):
        """åŠ è½½æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
        degree_as_tag = getattr(self.config, "degree_as_tag", False)
        self.all_graph_data, self.tagset, self.max_node_num, self.max_feat_dim = (
            load_from_file_optimized(self.config, degree_as_tag)
        )

        # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
        label_counts = defaultdict(int)
        for graph_data in self.all_graph_data:
            original_label = graph_data.get_label("original")
            label_counts[original_label] += 1

        # ä¼˜å…ˆå°è¯•åŠ è½½é¢„å®šä¹‰çš„ç±»åˆ«åˆ†å‰²
        predefined_train_classes, predefined_test_classes = self._load_predefined_class_splits()

        if predefined_train_classes is not None and predefined_test_classes is not None:
            # ä½¿ç”¨é¢„å®šä¹‰åˆ†å‰²
            available_labels = set(label_counts.keys())

            # éªŒè¯é¢„å®šä¹‰ç±»åˆ«æ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
            valid_train_classes = [c for c in predefined_train_classes if c in available_labels]
            valid_test_classes = [c for c in predefined_test_classes if c in available_labels]

            if valid_train_classes and valid_test_classes:
                self.train_classes = valid_train_classes
                self.test_classes = valid_test_classes
                self.train_class_num = len(self.train_classes)
                self.test_class_num = len(self.test_classes)

                print(f"âœ… ä½¿ç”¨é¢„å®šä¹‰ç±»åˆ«åˆ†å‰²:")
                print(f"  è®­ç»ƒç±»åˆ«æ•°: {self.train_class_num}, ç±»åˆ«: {self.train_classes}")
                print(f"  æµ‹è¯•ç±»åˆ«æ•°: {self.test_class_num}, ç±»åˆ«: {self.test_classes}")
            else:
                print(f"âš ï¸ é¢„å®šä¹‰ç±»åˆ«ä¸æ•°æ®ä¸åŒ¹é…ï¼Œå›é€€åˆ°è‡ªåŠ¨åˆ†å‰²")
                predefined_train_classes, predefined_test_classes = None, None

        # å¦‚æœæ²¡æœ‰é¢„å®šä¹‰åˆ†å‰²æˆ–é¢„å®šä¹‰åˆ†å‰²æ— æ•ˆï¼Œåˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
        if predefined_train_classes is None or predefined_test_classes is None:
            print(f"ğŸ“Š ä½¿ç”¨è‡ªåŠ¨ç±»åˆ«åˆ†å‰²...")

            # ç¡®å®šæ•°æ®åˆ†å‰²ç­–ç•¥
            if hasattr(self.config, "test_class_num") and hasattr(self.config, "train_class_num"):
                # æ˜¾å¼æŒ‡å®šè®­ç»ƒå’Œæµ‹è¯•ç±»åˆ«æ•°
                self.test_class_num = self.config.test_class_num
                self.train_class_num = self.config.train_class_num
            else:
                # æ ¹æ®test_splitæ¯”ä¾‹è‡ªåŠ¨åˆ†å‰²
                test_split = getattr(self.config, "test_split", 0.2)
                total_classes = len(label_counts)
                self.test_class_num = max(1, int(total_classes * test_split))
                self.train_class_num = total_classes - self.test_class_num

            # æ ¹æ®æ ·æœ¬æ•°é‡æ’åºï¼Œç¡®ä¿æµ‹è¯•ç±»æœ‰è¶³å¤Ÿæ ·æœ¬
            sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)

            # è‡ªåŠ¨åˆ†é…ç±»åˆ«
            self.test_classes = [label for label, _ in sorted_labels[: self.test_class_num]]
            self.train_classes = [
                label
                for label, _ in sorted_labels[
                    self.test_class_num : self.test_class_num + self.train_class_num
                ]
            ]

            print(f"  è®­ç»ƒç±»åˆ«æ•°: {self.train_class_num}, ç±»åˆ«: {self.train_classes}")
            print(f"  æµ‹è¯•ç±»åˆ«æ•°: {self.test_class_num}, ç±»åˆ«: {self.test_classes}")

        # ä¸ºå›¾æ•°æ®åˆ†é…è®­ç»ƒ/æµ‹è¯•æ ‡ç­¾
        train_label_map = {
            original_label: idx for idx, original_label in enumerate(self.train_classes)
        }
        test_label_map = {
            original_label: idx for idx, original_label in enumerate(self.test_classes)
        }

        self.train_graphs = []
        self.test_graphs = []

        for graph_data in self.all_graph_data:
            original_label = graph_data.get_label("original")
            if original_label in train_label_map:
                graph_data.set_train_split(train_label_map[original_label])
                self.train_graphs.append(graph_data)
            elif original_label in test_label_map:
                graph_data.set_test_split(test_label_map[original_label])
                self.test_graphs.append(graph_data)

    def _convert_to_tensors(self):
        """å°†å›¾æ•°æ®è½¬æ¢ä¸ºå¼ é‡æ ¼å¼"""
        print("Converting graphs to tensors...")

        def graphs_to_tensor_batch(graph_data_list):
            max_node_num = self.config.max_node_num
            max_feat_num = self.config.max_feat_num
            adj_tensor, x_tensor = graphs_to_tensor(
                [gd.nx_graph for gd in graph_data_list], max_node_num, max_feat_num
            )
            labels = torch.tensor(
                [
                    (
                        gd.get_label("train_split")
                        if gd.get_label("train_split") is not None
                        else gd.get_label("test_split")
                    )
                    for gd in graph_data_list
                ],
                dtype=torch.long,
            )
            return x_tensor, adj_tensor, labels

        self.train_x, self.train_adj, self.train_labels = graphs_to_tensor_batch(self.train_graphs)
        self.test_x, self.test_adj, self.test_labels = graphs_to_tensor_batch(self.test_graphs)

        print(f"âœ“ Tensor conversion completed:")
        print(f"  Train tensors: x{self.train_x.shape}, adj{self.train_adj.shape}")
        print(f"  Test tensors: x{self.test_x.shape}, adj{self.test_adj.shape}")

    def _build_fsl_indices(self):
        """æ„å»ºFSLä»»åŠ¡çš„ç´¢å¼•æ˜ å°„"""
        if not self.fsl_config:
            return

        # è®­ç»ƒé›†ç´¢å¼•ï¼ˆæŒ‰ç±»åˆ«ï¼‰
        self.train_indices_by_class = defaultdict(list)
        for idx, graph_data in enumerate(self.train_graphs):
            class_id = graph_data.get_label("train_split")
            self.train_indices_by_class[class_id].append(idx)

        # æµ‹è¯•é›†ç´¢å¼•ï¼ˆæŒ‰ç±»åˆ«ï¼‰
        self.test_indices_by_class = defaultdict(list)
        for idx, graph_data in enumerate(self.test_graphs):
            class_id = graph_data.get_label("test_split")
            self.test_indices_by_class[class_id].append(idx)

        # è®¡ç®—å„ç±»åˆ«å¯ç”¨æ ·æœ¬æ•°ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        if hasattr(self.fsl_config, "K_shot"):
            K_shot = self.fsl_config.K_shot
        else:
            K_shot = 1  # é»˜è®¤å€¼

        total_query_samples = 0
        for class_id in sorted(self.test_indices_by_class.keys()):
            class_indices = self.test_indices_by_class[class_id]
            remaining_count = max(0, len(class_indices) - K_shot)
            total_query_samples += remaining_count

        print(f"âœ“ FSLç´¢å¼•æ„å»ºå®Œæˆ: æ€»æŸ¥è¯¢æ ·æœ¬æ•°={total_query_samples}")

    def get_loaders(self):
        """è¿”å›è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        batch_size = getattr(self.config, "batch_size", 64)

        train_dataset = TensorDataset(self.train_x, self.train_adj, self.train_labels)
        test_dataset = TensorDataset(self.test_x, self.test_adj, self.test_labels)

        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, drop_last=False
        )
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False, drop_last=False
        )

        return train_loader, test_loader

    def _prepare_task_class_samples(self, all_indices_for_class_list, num_needed):
        """ä¸ºä»»åŠ¡å‡†å¤‡ç±»åˆ«æ ·æœ¬"""
        selected_indices = []
        for class_indices in all_indices_for_class_list:
            if len(class_indices) >= num_needed:
                selected = np.random.choice(class_indices, num_needed, replace=False).tolist()
            else:
                selected = (
                    class_indices
                    + np.random.choice(
                        class_indices, num_needed - len(class_indices), replace=True
                    ).tolist()
                )
            selected_indices.extend(selected)
        return selected_indices

    def _sample_indices_for_task(
        self, indices_map_by_class, list_of_available_classes, N_way, K_shot, R_query
    ):
        """ä¸ºFSLä»»åŠ¡é‡‡æ ·ç´¢å¼•"""
        # è¿‡æ»¤æ‰æ ·æœ¬æ•°ä¸è¶³çš„ç±»åˆ«
        valid_classes = [
            c for c in list_of_available_classes if len(indices_map_by_class.get(c, [])) > 0
        ]

        if len(valid_classes) < N_way:
            return None

        selected_classes = np.random.choice(valid_classes, N_way, replace=False)

        support_class_indices = []
        query_class_indices = []

        for class_id in selected_classes:
            class_indices = indices_map_by_class[class_id]
            total_needed = K_shot + R_query

            if len(class_indices) >= total_needed:
                selected_indices = np.random.choice(
                    class_indices, total_needed, replace=False
                ).tolist()
                support_indices = selected_indices[:K_shot]
                query_indices = selected_indices[K_shot:]
            else:
                # å¤„ç†æ ·æœ¬ä¸è¶³çš„æƒ…å†µ
                selected_indices = (
                    class_indices
                    + np.random.choice(
                        class_indices, total_needed - len(class_indices), replace=True
                    ).tolist()
                )
                support_indices = selected_indices[:K_shot]
                query_indices = selected_indices[K_shot : K_shot + R_query]

            support_class_indices.append(support_indices)
            query_class_indices.append(query_indices)

        return {
            "selected_classes": selected_classes,
            "support_indices": support_class_indices,
            "query_indices": query_class_indices,
        }

    def sample_one_task(self, is_train, N_way, K_shot, R_query, query_pool_start_index=None):
        """
        é‡‡æ ·ä¸€ä¸ªFSLä»»åŠ¡

        è®­ç»ƒæ—¶ï¼š
        - æ”¯æŒé›†ï¼šä»è®­ç»ƒç±»åˆ«ä¸­éšæœºé‡‡æ ·çš„ K ä¸ªæ ·æœ¬
        - æŸ¥è¯¢é›†ï¼šä»åŒä¸€ç±»åˆ«ä¸­éšæœºé‡‡æ ·çš„æ¥ä¸‹æ¥ R ä¸ªæ ·æœ¬
        - æ¯æ¬¡éƒ½é‡æ–°éšæœºæ‰“ä¹±ï¼Œç¡®ä¿å¤šæ ·æ€§

        æµ‹è¯•æ—¶ï¼š
        - æ”¯æŒé›†ï¼šä»æµ‹è¯•ç±»åˆ«ä¸­å›ºå®šå–å‰ K ä¸ªæ ·æœ¬
        - æŸ¥è¯¢é›†ï¼šä»é¢„å…ˆæ„å»ºçš„ total_test_g_listï¼ˆå…¨å±€æµ‹è¯•æ ·æœ¬æ± ï¼‰ä¸­æŒ‰åºå–æ ·æœ¬
        - ä¸é‡æ–°æ‰“ä¹±ï¼Œç¡®ä¿æµ‹è¯•çš„ä¸€è‡´æ€§

        Args:
            is_train: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            N_way: N-wayåˆ†ç±»
            K_shot: æ¯ç±»æ”¯æŒæ ·æœ¬æ•°
            R_query: æ¯ç±»æŸ¥è¯¢æ ·æœ¬æ•°ï¼ˆæŸ¥è¯¢é›†æ€»å¤§å°ä¸º N_way * R_queryï¼‰
            query_pool_start_index: æŸ¥è¯¢æ± èµ·å§‹ç´¢å¼•ï¼Œç”¨äºæµ‹è¯•æ¨¡å¼çš„å…¨å±€æ± é‡‡æ ·

        Returns:
            task: åŒ…å«support_setå’Œquery_setçš„ä»»åŠ¡
        """
        if is_train:
            # ==================== è®­ç»ƒæ¨¡å¼ ====================
            # æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†éƒ½ä»åŒä¸€ç±»åˆ«å†…éšæœºé‡‡æ ·
            indices_map = self.train_indices_by_class
            available_classes = list(range(len(self.train_classes)))
            x_tensor, adj_tensor = self.train_x, self.train_adj

            # è¿‡æ»¤æ‰æ ·æœ¬æ•°ä¸è¶³çš„ç±»åˆ«
            valid_classes = [
                c for c in available_classes if len(indices_map.get(c, [])) >= K_shot + R_query
            ]

            if len(valid_classes) < N_way:
                return None

            # éšæœºé€‰æ‹©N_wayä¸ªç±»åˆ«
            selected_classes = np.random.choice(valid_classes, N_way, replace=False)

            support_indices = []
            support_labels = []
            query_indices = []
            query_labels = []

            for class_idx, class_id in enumerate(selected_classes):
                class_indices = indices_map[class_id]
                total_needed = K_shot + R_query

                if len(class_indices) >= total_needed:
                    # éšæœºé‡‡æ ·K+Rä¸ªæ ·æœ¬
                    selected_indices = np.random.choice(
                        class_indices, total_needed, replace=False
                    ).tolist()
                    class_support_indices = selected_indices[:K_shot]
                    class_query_indices = selected_indices[K_shot:]
                else:
                    # æ ·æœ¬ä¸è¶³æ—¶ç”¨é‡å¤é‡‡æ ·
                    selected_indices = (
                        class_indices
                        + np.random.choice(
                            class_indices, total_needed - len(class_indices), replace=True
                        ).tolist()
                    )
                    class_support_indices = selected_indices[:K_shot]
                    class_query_indices = selected_indices[K_shot : K_shot + R_query]

                # æ·»åŠ åˆ°æ”¯æŒé›†
                support_indices.extend(class_support_indices)
                support_labels.extend([class_idx] * len(class_support_indices))

                # æ·»åŠ åˆ°æŸ¥è¯¢é›†
                query_indices.extend(class_query_indices)
                query_labels.extend([class_idx] * len(class_query_indices))

            # è®­ç»ƒæ¨¡å¼ä¸éœ€è¦å¡«å……æ ·æœ¬
            append_count = 0

        else:
            # ==================== æµ‹è¯•æ¨¡å¼ ====================
            # æ”¯æŒé›†å›ºå®šï¼ŒæŸ¥è¯¢é›†ä»å…¨å±€æ± æŒ‰åºå–æ ·
            indices_map = self.test_indices_by_class
            available_classes = list(range(len(self.test_classes)))
            x_tensor, adj_tensor = self.test_x, self.test_adj

            # è¿‡æ»¤æ‰æ ·æœ¬æ•°ä¸è¶³çš„ç±»åˆ«ï¼ˆæ”¯æŒé›†éœ€è¦ï¼‰
            valid_classes = [c for c in available_classes if len(indices_map.get(c, [])) >= K_shot]

            if len(valid_classes) < N_way:
                return None

            # å›ºå®šé€‰æ‹©å‰N_wayä¸ªæœ‰æ•ˆç±»åˆ«ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
            selected_classes = valid_classes[:N_way]

            # æ„å»ºå›ºå®šæ”¯æŒé›†
            support_indices = []
            support_labels = []

            for class_idx, class_id in enumerate(selected_classes):
                class_indices = indices_map[class_id]
                # å›ºå®šå–å‰K_shotä¸ªæ ·æœ¬ä½œä¸ºæ”¯æŒé›†
                class_support_indices = class_indices[:K_shot]
                support_indices.extend(class_support_indices)
                support_labels.extend([class_idx] * len(class_support_indices))

            # ä¿®å¤ï¼šæ­£ç¡®æ„å»ºæŸ¥è¯¢é›†ï¼Œç¡®ä¿æ ‡ç­¾åŒ¹é…
            query_pool_start = query_pool_start_index if query_pool_start_index is not None else 0

            query_indices = []
            query_labels = []
            append_count = 0

            # è®¡ç®—æ¯ä¸ªç±»åˆ«å¯ç”¨çš„æŸ¥è¯¢æ ·æœ¬
            available_query_samples = {}
            for class_idx, class_id in enumerate(selected_classes):
                class_indices = indices_map[class_id]
                # é™¤å»æ”¯æŒé›†åçš„å‰©ä½™æ ·æœ¬
                remaining_indices = class_indices[K_shot:]
                available_query_samples[class_idx] = remaining_indices

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æŸ¥è¯¢æ ·æœ¬
            min_available = min(len(samples) for samples in available_query_samples.values())
            required_per_class = R_query

            # è®¡ç®—å¯ä»¥é‡‡æ ·çš„æœ€å¤§ä»»åŠ¡æ•°
            if min_available == 0:
                return None  # æ²¡æœ‰æŸ¥è¯¢æ ·æœ¬å¯ç”¨

            # è®¡ç®—å½“å‰ä»»åŠ¡çš„æŸ¥è¯¢é›†èµ·å§‹åç§»
            max_possible_tasks = min_available // required_per_class
            current_task_offset = query_pool_start // (N_way * R_query)

            if current_task_offset >= max_possible_tasks:
                return None  # å·²ç»è¶…å‡ºå¯ç”¨ä»»åŠ¡æ•°

            # ä¸ºæ¯ä¸ªç±»åˆ«é‡‡æ ·æŸ¥è¯¢æ ·æœ¬
            for class_idx, class_id in enumerate(selected_classes):
                available_indices = available_query_samples[class_idx]

                # è®¡ç®—å½“å‰ç±»åˆ«çš„é‡‡æ ·èµ·å§‹ä½ç½®
                start_offset = current_task_offset * required_per_class
                end_offset = start_offset + required_per_class

                if start_offset >= len(available_indices):
                    # æ²¡æœ‰è¶³å¤Ÿæ ·æœ¬ï¼Œè¿”å›Noneåœæ­¢é‡‡æ ·
                    return None

                # å–å‡ºå½“å‰ä»»åŠ¡éœ€è¦çš„æŸ¥è¯¢æ ·æœ¬
                if end_offset <= len(available_indices):
                    class_query_indices = available_indices[start_offset:end_offset]
                else:
                    # æ ·æœ¬ä¸è¶³ï¼Œç”¨é‡å¤å¡«å……
                    class_query_indices = available_indices[start_offset:]
                    while len(class_query_indices) < required_per_class:
                        if len(class_query_indices) > 0:
                            class_query_indices.append(class_query_indices[-1])
                            append_count += 1
                        else:
                            # å¦‚æœå®Œå…¨æ²¡æœ‰æ ·æœ¬ï¼Œè¿”å›None
                            return None

                # æ·»åŠ åˆ°æŸ¥è¯¢é›†
                query_indices.extend(class_query_indices[:required_per_class])
                query_labels.extend([class_idx] * required_per_class)

        # æå–å¼ é‡æ•°æ®
        support_x = x_tensor[support_indices]
        support_adj = adj_tensor[support_indices]
        support_labels_tensor = torch.tensor(support_labels, dtype=torch.long)

        query_x = x_tensor[query_indices]
        query_adj = adj_tensor[query_indices]
        query_labels_tensor = torch.tensor(query_labels, dtype=torch.long)

        return {
            "support_set": {
                "x": support_x,
                "adj": support_adj,
                "label": support_labels_tensor,
            },
            "query_set": {
                "x": query_x,
                "adj": query_adj,
                "label": query_labels_tensor,
            },
            "N_way": N_way,
            "K_shot": K_shot,
            "R_query": R_query,
            "selected_classes": selected_classes,
            "append_count": append_count,
        }

    def _print_dataset_summary(self):
        """æ‰“å°æ•°æ®é›†æ‘˜è¦"""
        pass

    def _load_predefined_class_splits(self):
        """åŠ è½½é¢„å®šä¹‰çš„ç±»åˆ«åˆ†å‰²æ–‡ä»¶"""
        split_file_path = f"datasets/{self.dataset_name}/train_test_classes.json"

        if os.path.exists(split_file_path):
            try:
                import json

                with open(split_file_path, "r") as f:
                    splits = json.load(f)
                    train_classes = splits.get("train", [])
                    test_classes = splits.get("test", [])

                return train_classes, test_classes
            except Exception as e:
                return None, None
        else:
            return None, None


def load_from_file(data_config, use_degree_as_tag):
    """å‘åå…¼å®¹å‡½æ•°"""
    warnings.warn(
        "load_from_file is deprecated, use load_from_file_optimized instead",
        DeprecationWarning,
        stacklevel=2,
    )
    return load_from_file_optimized(data_config, use_degree_as_tag)


def load_data(config, get_graph_list=False):
    """å‘åå…¼å®¹çš„æ•°æ®åŠ è½½å‡½æ•°"""
    fsl_task_config = getattr(config, "fsl_task", None)
    dataset = MyDatasetOptimized(config.data, fsl_task_config)

    if get_graph_list:
        return dataset, dataset.all_graph_data
    else:
        return dataset


class DataLoadingProfiler:
    """æ•°æ®åŠ è½½æ€§èƒ½åˆ†æå·¥å…·"""

    @staticmethod
    def profile_data_loading(data_config, fsl_task_config=None, num_runs=3):
        """åˆ†ææ•°æ®åŠ è½½æ€§èƒ½"""
        times = []

        for i in range(num_runs):
            start_time = time.time()
            dataset_old = MyDatasetOptimized(data_config, fsl_task_config)
            end_time = time.time()
            times.append(end_time - start_time)
            print(f"Run {i+1}: {times[-1]:.2f}s")

        avg_time = np.mean(times)
        std_time = np.std(times)

        print(f"\nAverage loading time: {avg_time:.2f}s Â± {std_time:.2f}s")

        # å†…å­˜ä½¿ç”¨åˆ†æ
        print(f"Train graphs: {len(dataset_old.train_graphs)}")
        print(f"Test graphs: {len(dataset_old.test_graphs)}")

        # åˆ†ææ•°æ®è´¨é‡
        train_sizes = [len(indices) for indices in dataset_old.train_indices_by_class.values()]
        test_sizes = [len(indices) for indices in dataset_old.test_indices_by_class.values()]

        print(
            f"Train class sizes: min={min(train_sizes)}, max={max(train_sizes)}, avg={np.mean(train_sizes):.1f}"
        )
        print(
            f"Test class sizes: min={min(test_sizes)}, max={max(test_sizes)}, avg={np.mean(test_sizes):.1f}"
        )

        # é‡‡æ ·ä»»åŠ¡æµ‹è¯•
        if fsl_task_config:
            start_time = time.time()
            for _ in range(10):
                task = dataset_old.sample_one_task(
                    is_train=False,
                    N_way=fsl_task_config.N_way,
                    K_shot=fsl_task_config.K_shot,
                    R_query=fsl_task_config.R_query,
                )
            end_time = time.time()
            print(f"10 FSL task sampling time: {end_time - start_time:.2f}s")

        return dataset_old


class GraphData:
    """
    å›¾æ•°æ®åŒ…è£…ç±»ï¼Œæ”¯æŒåŸå§‹æ ‡ç­¾å’Œè®­ç»ƒ/æµ‹è¯•æ ‡ç­¾
    """

    def __init__(self, nx_graph, original_label):
        self.nx_graph = nx_graph
        self.original_label = original_label
        self.train_split_label = None
        self.test_split_label = None

    def __repr__(self):
        return (
            f"GraphData(nodes={self.nx_graph.number_of_nodes()}, "
            f"edges={self.nx_graph.number_of_edges()}, "
            f"original_label={self.original_label})"
        )

    def get_label(self, mode="original"):
        """
        è·å–æ ‡ç­¾
        mode: "original", "train_split", "test_split"
        """
        if mode == "original":
            return self.original_label
        elif mode == "train_split":
            return self.train_split_label
        elif mode == "test_split":
            return self.test_split_label
        else:
            raise ValueError(f"Unknown label mode: {mode}")

    def set_train_split(self, remapped_label):
        """è®¾ç½®è®­ç»ƒåˆ†å‰²æ ‡ç­¾"""
        self.train_split_label = remapped_label

    def set_test_split(self, remapped_label):
        """è®¾ç½®æµ‹è¯•åˆ†å‰²æ ‡ç­¾"""
        self.test_split_label = remapped_label


# ä¸ºäº†å‘åå…¼å®¹ï¼Œåˆ›å»ºMyDatasetåˆ«å
MyDataset = MyDatasetOptimized

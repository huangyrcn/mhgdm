"""
é«˜æ•ˆæ•°æ®å·¥å…· - åŠ¨æ€æ‰¹å¤„ç†ï¼Œæ¶ˆé™¤å†…å­˜æµªè´¹
"""

import os
import pathlib
import pickle
import networkx as nx
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader, TensorDataset
import json
from collections import defaultdict
from numpy.random import RandomState
import warnings


def graphs_to_tensor_robust(graph_list, max_node_num, max_feat_num):
    """
    å¥å£®çš„å›¾è½¬å¼ é‡å‡½æ•° - æ”¯æŒtagå’Œfeatureå±æ€§
    """
    adjs_list = []
    x_list = []

    for g in graph_list:
        assert isinstance(g, nx.Graph)

        # è·å–èŠ‚ç‚¹åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºï¼‰
        node_list = list(g.nodes())

        # æ„å»ºé‚»æ¥çŸ©é˜µ
        adj = nx.to_numpy_array(g, nodelist=node_list)

        # å¡«å……é‚»æ¥çŸ©é˜µ
        adj = _pad_adjs(adj, max_node_num)
        adjs_list.append(adj)

        # æ„å»ºç‰¹å¾çŸ©é˜µ
        feature_list = []
        for node in node_list:
            node_data = g.nodes[node]

            if "feature" in node_data:
                # ä½¿ç”¨æ˜¾å¼ç‰¹å¾
                features = node_data["feature"]
                if isinstance(features, (list, np.ndarray)):
                    feature_list.append(np.array(features, dtype=np.float32))
                else:
                    feature_list.append(np.array([features], dtype=np.float32))
            elif "tag" in node_data:
                # ä½¿ç”¨one-hotç¼–ç çš„åº¦æ•°æ ‡ç­¾
                tag = node_data["tag"]
                one_hot = np.zeros(max_feat_num, dtype=np.float32)
                if tag < max_feat_num:
                    one_hot[tag] = 1.0
                feature_list.append(one_hot)
            else:
                # é»˜è®¤ä½¿ç”¨åº¦æ•°ä½œä¸ºç‰¹å¾
                degree = g.degree(node)
                one_hot = np.zeros(max_feat_num, dtype=np.float32)
                if degree < max_feat_num:
                    one_hot[degree] = 1.0
                feature_list.append(one_hot)

        # è½¬æ¢ä¸ºæ•°ç»„å¹¶å¡«å……
        if feature_list:
            x = np.stack(feature_list, axis=0)
            x = _pad_features(x, max_node_num, max_feat_num)
            x_list.append(x)

    adjs_tensor = torch.tensor(np.asarray(adjs_list), dtype=torch.float32)
    x_tensor = torch.tensor(np.asarray(x_list), dtype=torch.float32)

    return adjs_tensor, x_tensor


def _pad_adjs(adj, node_number):
    """å¡«å……é‚»æ¥çŸ©é˜µåˆ°æŒ‡å®šå¤§å°"""
    a = adj
    ori_len = a.shape[0]
    if ori_len == node_number:
        return a
    if ori_len > node_number:
        raise ValueError(f"ori_len {ori_len} > node_number {node_number}")
    a = np.concatenate([a, np.zeros([ori_len, node_number - ori_len])], axis=-1)
    a = np.concatenate([a, np.zeros([node_number - ori_len, node_number])], axis=0)
    return a


def _pad_features(x, node_number, feature_dim):
    """å¡«å……ç‰¹å¾çŸ©é˜µåˆ°æŒ‡å®šå¤§å°"""
    n_nodes, feat_dim = x.shape
    if n_nodes > node_number:
        raise ValueError(f"n_nodes {n_nodes} > node_number {node_number}")
    if feat_dim > feature_dim:
        raise ValueError(f"feat_dim {feat_dim} > feature_dim {feature_dim}")

    padded_x = np.zeros((node_number, feature_dim), dtype=x.dtype)
    padded_x[:n_nodes, :feat_dim] = x
    return padded_x


def load_from_file(data_config, use_degree_as_tag):
    """
    æ•°æ®åŠ è½½å‡½æ•°ï¼Œå¢å¼ºäº†é”™è¯¯å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–
    """
    dataset_name = data_config.name
    base_dir = pathlib.Path("./datasets") / dataset_name
    file_path = base_dir / f"{dataset_name}.txt"
    save_file_path = base_dir / f"{dataset_name}_processed.pkl"
    force_reload = getattr(data_config, "force_reload_data", False)

    if not force_reload and save_file_path.exists():
        print(f"Loading processed data from: {save_file_path}")
        try:
            with open(save_file_path, "rb") as f:
                data = pickle.load(f)
            return data["all_nx_graphs"], data["tagset"], data["max_node_num"], data["max_feat_dim"]
        except (ModuleNotFoundError, AttributeError, ImportError) as e:
            if "numpy._core" in str(e) or "numpy.core" in str(e):
                print(f"âš ï¸  Numpy version compatibility issue detected: {e}")
                print("ğŸ”§ Regenerating processed data file with current numpy version...")
                force_reload = True
            else:
                raise e
        except Exception as e:
            print(f"âš ï¸  Error loading processed data: {e}")
            print("ğŸ”§ Regenerating processed data file...")
            force_reload = True

    if force_reload or not save_file_path.exists():
        print(f"Processing raw data from: {file_path}")
        if not file_path.exists():
            raise FileNotFoundError(f"Dataset file not found: {file_path}")

        all_nx_graphs = []
        tagset = set()

        try:
            with open(file_path, "r") as f:
                n_graphs = int(f.readline().strip())
                print(f"Loading {n_graphs} graphs...")

                for i in range(n_graphs):
                    line = f.readline().strip().split()
                    n_nodes, graph_label = int(line[0]), int(line[1])

                    g = nx.Graph()
                    g.graph["label"] = graph_label

                    for j in range(n_nodes):
                        node_line = f.readline().strip().split()
                        node_id = int(node_line[0])
                        n_neighbors = int(node_line[1])

                        # å¤„ç†èŠ‚ç‚¹ç‰¹å¾
                        feature_start_idx = 2 + n_neighbors
                        if len(node_line) > feature_start_idx:
                            # æœ‰æ˜¾å¼ç‰¹å¾
                            features = [float(x) for x in node_line[feature_start_idx:]]
                            g.add_node(node_id, features=features)
                        else:
                            # ä½¿ç”¨åº¦æ•°ä½œä¸ºç‰¹å¾
                            if use_degree_as_tag:
                                g.add_node(node_id, tag=n_neighbors)
                                tagset.add(n_neighbors)
                            else:
                                g.add_node(node_id, features=[1.0])

                        # æ·»åŠ è¾¹
                        for k in range(2, 2 + n_neighbors):
                            neighbor = int(node_line[k])
                            g.add_edge(node_id, neighbor)

                    all_nx_graphs.append(g)

                    if (i + 1) % 1000 == 0:
                        print(f"  Processed {i + 1}/{n_graphs} graphs")

        except Exception as e:
            print(f"Error reading dataset file: {e}")
            raise

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        max_node_num = max(len(g.nodes()) for g in all_nx_graphs)

        # è®¡ç®—ç‰¹å¾ç»´åº¦
        max_feat_dim = 1  # é»˜è®¤å€¼
        for g in all_nx_graphs:
            for node in g.nodes():
                node_data = g.nodes[node]
                if "feature" in node_data:
                    features = node_data["feature"]
                    if isinstance(features, (list, np.ndarray)):
                        feat_dim = len(features)
                    else:
                        feat_dim = 1
                    max_feat_dim = max(max_feat_dim, feat_dim)
                    break

        if use_degree_as_tag and tagset:
            max_feat_dim = len(tagset)

        print(f"Dataset statistics:")
        print(f"  Total graphs: {len(all_nx_graphs)}")
        print(f"  Max nodes: {max_node_num}")
        print(f"  Max feature dim: {max_feat_dim}")
        print(f"  Tagset size: {len(tagset)}")

        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        try:
            save_data = {
                "all_nx_graphs": all_nx_graphs,
                "tagset": tagset,
                "max_node_num": max_node_num,
                "max_feat_dim": max_feat_dim,
            }
            with open(save_file_path, "wb") as f:
                pickle.dump(save_data, f)
            print(f"âœ“ Processed data saved to: {save_file_path}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not save processed data: {e}")

        return all_nx_graphs, tagset, max_node_num, max_feat_dim


class MyDataset:
    """
    é«˜æ•ˆæ•°æ®ç®¡ç†å™¨ - é¢„å¤„ç†å¼ é‡åŒ–æ¨¡å¼
    åœ¨åˆå§‹åŒ–æ—¶ä¸€æ¬¡æ€§å°†æ‰€æœ‰NetworkXå›¾è½¬æ¢ä¸ºå¼ é‡ï¼Œç„¶åä½¿ç”¨TensorDataset
    æä¾›ä¸‰ä¸ªä¸»è¦æ¥å£ï¼š
    1. get_loaders() - è·å–DataLoaderï¼ˆä½¿ç”¨é¢„å¤„ç†å¼ é‡ï¼‰
    2. get_graph_lists() - è·å–åŸå§‹å›¾åˆ—è¡¨
    3. sample_one_task() - è·å–FSLä»»åŠ¡
    """

    def __init__(self, data_config, fsl_task_config=None):
        self.data_config = data_config
        self.fsl_task_config = fsl_task_config
        use_degree_as_tag = getattr(data_config, "degree_as_tag", True)

        print(f"Initializing MyDataset for {data_config.name}...")
        all_nx_graphs, self.tagset, max_nodes_data, feat_dim_data = load_from_file(
            data_config, use_degree_as_tag
        )

        self.max_node_num = getattr(data_config, "max_node_num", max_nodes_data)
        # å…¼å®¹ä¸åŒçš„ç‰¹å¾ç»´åº¦å­—æ®µå
        if hasattr(data_config, "max_feat_dim"):
            self.max_feat_num = getattr(data_config, "max_feat_dim")
        elif hasattr(data_config, "max_feat_num"):
            self.max_feat_num = getattr(data_config, "max_feat_num")
        else:
            self.max_feat_num = len(self.tagset) if len(self.tagset) > 0 else feat_dim_data

        # æ•°æ®é›†åˆ’åˆ†
        split_file = pathlib.Path(f"./datasets/{data_config.name}/train_test_classes.json")
        with open(split_file, "r") as f:
            class_splits = json.load(f)
            self.train_original_classes_set = set(map(int, class_splits["train"]))
            self.test_original_classes_set = set(map(int, class_splits["test"]))

        # æŒ‰è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å‰²æ•°æ®
        self.train_nx_graphs = []
        train_orig_labels_list = []
        self.test_nx_graphs = []
        test_orig_labels_list = []

        for nx_g in all_nx_graphs:
            original_label = nx_g.graph["label"]
            if original_label in self.train_original_classes_set:
                self.train_nx_graphs.append(nx_g)
                train_orig_labels_list.append(original_label)
            elif original_label in self.test_original_classes_set:
                self.test_nx_graphs.append(nx_g)
                test_orig_labels_list.append(original_label)

        # æ ‡ç­¾é‡æ˜ å°„
        unique_train_sorted = sorted(list(self.train_original_classes_set))
        train_remapper = {orig_lbl: i for i, orig_lbl in enumerate(unique_train_sorted)}
        self.train_labels_remapped = torch.LongTensor(
            [train_remapper[l] for l in train_orig_labels_list]
        )
        self.num_train_classes_remapped = len(unique_train_sorted)

        unique_test_sorted = sorted(list(self.test_original_classes_set))
        test_remapper = {orig_lbl: i for i, orig_lbl in enumerate(unique_test_sorted)}
        self.test_labels_remapped = torch.LongTensor(
            [test_remapper[l] for l in test_orig_labels_list]
        )
        self.num_test_classes_remapped = len(unique_test_sorted)

        # è®¾ç½®å›¾åˆ—è¡¨æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰
        self.train_graphs = self.train_nx_graphs
        self.test_graphs = self.test_nx_graphs

        print(f"âœ“ Dataset loaded:")
        print(f"  Train graphs: {len(self.train_nx_graphs)}")
        print(f"  Test graphs: {len(self.test_nx_graphs)}")
        print(f"  Train classes: {self.num_train_classes_remapped}")
        print(f"  Test classes: {self.num_test_classes_remapped}")
        print(f"  Max nodes: {self.max_node_num}")
        print(f"  Max features: {self.max_feat_num}")

        # **å…³é”®æ”¹åŠ¨ï¼šä¸€æ¬¡æ€§é¢„å¤„ç†æ‰€æœ‰å›¾ä¸ºå¼ é‡**
        print("Converting graphs to tensors...")
        # æ³¨æ„ï¼šgraphs_to_tensor_robustè¿”å›(adjs_tensor, x_tensor)
        train_adjs, train_x = graphs_to_tensor_robust(
            self.train_nx_graphs, self.max_node_num, self.max_feat_num
        )
        test_adjs, test_x = graphs_to_tensor_robust(
            self.test_nx_graphs, self.max_node_num, self.max_feat_num
        )

        # æ­£ç¡®å­˜å‚¨ï¼šxæ˜¯ç‰¹å¾ï¼Œadjæ˜¯é‚»æ¥çŸ©é˜µ
        self.train_x = train_x
        self.train_adj = train_adjs
        self.test_x = test_x
        self.test_adj = test_adjs

        print(f"âœ“ Tensor conversion completed:")
        print(f"  Train tensors: x{self.train_x.shape}, adj{self.train_adj.shape}")
        print(f"  Test tensors: x{self.test_x.shape}, adj{self.test_adj.shape}")

        # FSLç´¢å¼•æ„å»º
        if self.fsl_task_config is not None:
            self._build_fsl_indices()

    def _build_fsl_indices(self):
        """æ„å»ºFSLä»»åŠ¡æ‰€éœ€çš„ç´¢å¼•"""
        # æ„å»ºç±»åˆ«åˆ°æ ·æœ¬ç´¢å¼•çš„æ˜ å°„
        self.train_indices_by_class = defaultdict(list)
        for i, remapped_label in enumerate(self.train_labels_remapped.tolist()):
            self.train_indices_by_class[remapped_label].append(i)

        self.test_indices_by_class = defaultdict(list)
        for i, remapped_label in enumerate(self.test_labels_remapped.tolist()):
            self.test_indices_by_class[remapped_label].append(i)

        # ç¡®å®šæ€§æµ‹è¯•é‡‡æ ·æ”¯æŒé›†ä¸æŸ¥è¯¢æ± 
        rng = np.random.RandomState(42)
        K_shot = getattr(self.fsl_task_config, "K_shot", 1)
        self.deterministic_test_support_indices_by_class = {}
        query_pool = []
        for cls, indices in self.test_indices_by_class.items():
            idxs = list(indices)
            rng.shuffle(idxs)
            self.deterministic_test_support_indices_by_class[cls] = idxs[:K_shot]
            query_pool.extend(idxs[K_shot:])
        rng.shuffle(query_pool)
        self.deterministic_test_query_pool_indices = query_pool

    # =============================================================================
    # ä¸‰å¤§ä¸»è¦æ¥å£
    # =============================================================================

    def get_loaders(self):
        """
        æ¥å£1ï¼šè·å–DataLoader - ä½¿ç”¨é¢„å¤„ç†å¼ é‡å’ŒTensorDataset
        """
        batch_size = getattr(self.data_config, "batch_size", 32)
        num_workers = getattr(self.data_config, "num_workers", 0)

        # **å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨TensorDatasetï¼Œä¸å†ä½¿ç”¨åŠ¨æ€collate**
        # æ³¨æ„é¡ºåºï¼šä¸load_batchå‡½æ•°æœŸæœ›çš„(x, adj, labels)ä¿æŒä¸€è‡´
        train_dataset = TensorDataset(self.train_x, self.train_adj, self.train_labels_remapped)
        test_dataset = TensorDataset(self.test_x, self.test_adj, self.test_labels_remapped)

        # **å…³é”®æ”¹åŠ¨ï¼šä¸ä¼ collate_fnï¼Œä½¿ç”¨é»˜è®¤collate**
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,  # å¯ä»¥å¼€å¯pin_memoryæå‡æ€§èƒ½
        )
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
        )

        return train_loader, test_loader

    def get_graph_lists(self):
        """
        æ¥å£2ï¼šè·å–åŸå§‹å›¾åˆ—è¡¨
        """
        return self.train_nx_graphs, self.test_nx_graphs

    def sample_one_task(self, is_train, N_way, K_shot, R_query, query_pool_start_index=None):
        """
        æ¥å£3ï¼šé‡‡æ ·FSLä»»åŠ¡ - ç›´æ¥ä»é¢„å¤„ç†çš„å¼ é‡ä¸­é‡‡æ ·
        """
        if self.fsl_task_config is None:
            raise RuntimeError("FSL task sampling requires fsl_task_config")

        # é€‰æ‹©æ•°æ®æº
        if is_train:
            indices_by_class = self.train_indices_by_class
            x_tensor = self.train_x
            adj_tensor = self.train_adj
            labels_tensor = self.train_labels_remapped
            num_classes = self.num_train_classes_remapped
        else:
            indices_by_class = self.test_indices_by_class
            x_tensor = self.test_x
            adj_tensor = self.test_adj
            labels_tensor = self.test_labels_remapped
            num_classes = self.num_test_classes_remapped

        # æ£€æŸ¥å¯ç”¨ç±»åˆ«æ•°
        available_classes = [
            cls for cls, indices in indices_by_class.items() if len(indices) >= K_shot
        ]
        if len(available_classes) < N_way:
            print(f"Warning: Only {len(available_classes)} classes available, but N_way={N_way}")
            N_way = len(available_classes)

        # éšæœºé€‰æ‹©N_wayä¸ªç±»åˆ«
        rng = RandomState(None)  # æ¯æ¬¡è°ƒç”¨éƒ½ä½¿ç”¨æ–°çš„éšæœºçŠ¶æ€
        selected_classes = rng.choice(available_classes, N_way, replace=False)

        # é‡‡æ ·æ”¯æŒé›†
        support_indices = []
        support_labels = []
        used_support_indices = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„æ”¯æŒé›†ç´¢å¼•

        for i, cls in enumerate(selected_classes):
            class_indices = indices_by_class[cls]
            selected_indices = rng.choice(class_indices, K_shot, replace=False)
            used_support_indices.update(selected_indices)  # è®°å½•æ”¯æŒé›†ç´¢å¼•

            for idx in selected_indices:
                support_indices.append(idx)
                support_labels.append(i)  # é‡æ˜ å°„ä¸º0, 1, 2, ...

        # é‡‡æ ·æŸ¥è¯¢é›†
        query_indices = []
        query_labels = []

        if is_train:
            # è®­ç»ƒæ—¶ï¼šä»æ¯ä¸ªé€‰ä¸­ç±»åˆ«çš„å‰©ä½™æ ·æœ¬ä¸­éšæœºé‡‡æ ·
            for i, cls in enumerate(selected_classes):
                class_indices = indices_by_class[cls]
                # æ’é™¤å·²ç”¨ä½œæ”¯æŒé›†çš„æ ·æœ¬
                available_indices = [
                    idx for idx in class_indices if idx not in used_support_indices
                ]

                if len(available_indices) >= R_query:
                    selected_query_indices = rng.choice(available_indices, R_query, replace=False)
                    for idx in selected_query_indices:
                        query_indices.append(idx)
                        query_labels.append(i)  # ä½¿ç”¨ä»»åŠ¡å†…çš„ç±»åˆ«æ ‡ç­¾ (0, 1, 2, ...)
        else:
            # æµ‹è¯•æ—¶ï¼šä½¿ç”¨é¢„å®šä¹‰çš„æŸ¥è¯¢æ± ï¼Œä½†éœ€è¦æ­£ç¡®æ˜ å°„æ ‡ç­¾
            if query_pool_start_index is None:
                query_pool_start_index = 0

            query_pool_indices = self.deterministic_test_query_pool_indices
            total_query_needed = N_way * R_query
            end_index = min(query_pool_start_index + total_query_needed, len(query_pool_indices))

            if end_index - query_pool_start_index < total_query_needed:
                warnings.warn(
                    f"Not enough samples in query pool. "
                    f"Requested: {total_query_needed}, Available: {end_index - query_pool_start_index}"
                )

            selected_query_pool = query_pool_indices[query_pool_start_index:end_index]

            # **å…³é”®ä¿®å¤ï¼šæ ¹æ®æ ·æœ¬çš„å®é™…ç±»åˆ«æ˜ å°„æ ‡ç­¾ï¼Œè€Œä¸æ˜¯æŒ‰é¡ºåºåˆ†é…**
            for idx in selected_query_pool:
                # è·å–æ ·æœ¬çš„åŸå§‹ç±»åˆ«
                sample_original_class = labels_tensor[idx].item()

                # æ£€æŸ¥è¿™ä¸ªç±»åˆ«æ˜¯å¦åœ¨å½“å‰ä»»åŠ¡çš„selected_classesä¸­
                if sample_original_class in selected_classes:
                    # æ‰¾åˆ°åœ¨selected_classesä¸­çš„ä½ç½®ï¼Œä½œä¸ºä»»åŠ¡å†…æ ‡ç­¾
                    task_label = list(selected_classes).index(sample_original_class)
                    query_indices.append(idx)
                    query_labels.append(task_label)

                    # å¦‚æœå·²ç»æ”¶é›†è¶³å¤Ÿçš„æŸ¥è¯¢æ ·æœ¬ï¼Œå°±åœæ­¢
                    if len(query_labels) >= total_query_needed:
                        break

        # **å…³é”®æ”¹åŠ¨ï¼šç›´æ¥ä»é¢„å¤„ç†çš„å¼ é‡ä¸­ç´¢å¼•ï¼Œä¸å†éœ€è¦å®æ—¶è½¬æ¢å›¾**
        support_x = x_tensor[support_indices]
        support_adj = adj_tensor[support_indices]
        support_labels_tensor = torch.LongTensor(support_labels)

        query_x = x_tensor[query_indices] if query_indices else torch.empty(0, *x_tensor.shape[1:])
        query_adj = (
            adj_tensor[query_indices] if query_indices else torch.empty(0, *adj_tensor.shape[1:])
        )
        query_labels_tensor = (
            torch.LongTensor(query_labels) if query_labels else torch.empty(0, dtype=torch.long)
        )

        return {
            "support_set": {
                "x": support_x,
                "adj": support_adj,
                "label": support_labels_tensor,
            },
            "query_set": {
                "x": query_x,
                "adj": query_adj,
                "label": query_labels_tensor,
            },
            "N_way": N_way,
        }


# --- å‘åå…¼å®¹æ€§åˆ«å ---
DataManager = MyDataset


def load_data(config, get_graph_list=False):
    """
    å‘åå…¼å®¹çš„æ•°æ®åŠ è½½å‡½æ•°
    ç°åœ¨ä½¿ç”¨é¢„å¤„ç†å¼ é‡åŒ–çš„ MyDataset
    """
    dataset = MyDataset(config.data, getattr(config, "fsl_task", None))

    if get_graph_list:
        return dataset.get_graph_lists()
    else:
        return dataset.get_loaders()


# æ•°æ®åŠ è½½åˆ«åï¼ˆå‘åå…¼å®¹ï¼‰
load_dataset = load_data

# --- å‘åå…¼å®¹æ€§è¯´æ˜ ---
"""
é‡è¦æ›´æ–°ï¼šMyDataset å·²å‡çº§ä¸ºé¢„å¤„ç†å¼ é‡åŒ–æ¨¡å¼

ä¸»è¦å˜åŒ–ï¼š
1. âœ“ åˆ é™¤äº†åŠ¨æ€ collate åŠŸèƒ½ï¼ˆsmart_collate_fnï¼‰
2. âœ“ åˆ é™¤äº† GraphDataset ç±»
3. âœ“ åœ¨åˆå§‹åŒ–æ—¶ä¸€æ¬¡æ€§é¢„å¤„ç†æ‰€æœ‰å›¾ä¸ºå¼ é‡
4. âœ“ ä½¿ç”¨ TensorDataset æ›¿ä»£åŠ¨æ€æ•°æ®é›†
5. âœ“ DataLoader ä½¿ç”¨é»˜è®¤ collateï¼Œä¸ä¼ å…¥ collate_fn
6. âœ“ ä¿æŒ (x, adj, label) ä¸‰å…ƒè¾“å‡ºé¡ºåº

æ€§èƒ½ä¼˜åŠ¿ï¼š
- å†…å­˜é¢„åˆ†é…ï¼Œé¿å…åŠ¨æ€è®¡ç®—æ‰¹æ¬¡æœ€å¤§å°ºå¯¸
- æ¶ˆé™¤å®æ—¶å›¾è½¬æ¢å¼€é”€
- æ”¯æŒ pin_memory å’Œå¤šè¿›ç¨‹åŠ è½½
- æ›´å¥½çš„æ•°æ®å±€éƒ¨æ€§å’Œç¼“å­˜å‹å¥½æ€§

æ¥å£å…¼å®¹æ€§ï¼š
- get_loaders() æ¥å£ä¿æŒä¸å˜
- load_batch() å‡½æ•°æ— éœ€ä¿®æ”¹
- æ¨¡å‹å‰å‘ä¼ æ’­æ¥å£ä¿æŒä¸å˜
- FSL ä»»åŠ¡é‡‡æ ·æ€§èƒ½å¤§å¹…æå‡ï¼ˆç›´æ¥å¼ é‡ç´¢å¼•ï¼‰
"""

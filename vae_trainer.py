"""
VAEè®­ç»ƒå‡½æ•° - ç®€åŒ–ç‰ˆæœ¬
æ”¯æŒåŒæ›²å›¾è‡ªç¼–ç å™¨è®­ç»ƒï¼Œé›†æˆå…ƒæµ‹è¯•ç›‘æ§
"""

import os
import sys
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import wandb
from tqdm import trange, tqdm
from omegaconf import OmegaConf

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.config_utils import load_config, save_config
from utils.data_utils import MyDataset
from utils.loader import load_seed, load_device, load_batch, load_model
from utils.graph_utils import node_flags
from models.GraphVAE import GraphVAE
from models.Decoders import Classifier
import torch.nn.functional as F


class EarlyStopping:
    """æ—©åœæœºåˆ¶ - ç›‘æ§Meta-Testå‡†ç¡®ç‡"""

    def __init__(self, patience=5, min_delta=0.01, mode="max"):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.best_score = None
        self.counter = 0
        self.early_stop = False

    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
            return False

        if self.mode == "max":
            improved = score > self.best_score + self.min_delta
        else:
            improved = score < self.best_score - self.min_delta

        if improved:
            self.best_score = score
            self.counter = 0
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                return True
            return False


def train_vae(config):
    """
    VAEè®­ç»ƒä¸»å‡½æ•°

    Args:
        config: é…ç½®å¯¹è±¡

    Returns:
        dict: è®­ç»ƒç»“æœï¼ŒåŒ…å«æœ€ä½³æ£€æŸ¥ç‚¹è·¯å¾„ç­‰ä¿¡æ¯
    """
    # ç¡®ä¿wandbä¼šè¯å¹²å‡€
    try:
        if wandb.run is not None:
            wandb.finish()
            print("âœ“ å·²å…³é—­ä¹‹å‰çš„wandbä¼šè¯")
    except:
        pass

    # åŸºç¡€è®¾ç½®
    device = load_device(config)
    load_seed(config.seed)

    # åˆå§‹åŒ–wandb
    _init_wandb(config)

    # åŠ è½½æ•°æ®é›†
    dataset = MyDataset(config.data, config.fsl_task)
    train_loader, test_loader = dataset.get_loaders()

    # åˆå§‹åŒ–æ¨¡å‹
    model, optimizer, scheduler, warmup_scheduler, use_warmup = _init_model(config, device)

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = _create_save_dir(config)

    tqdm.write(f"VAEè®­ç»ƒåˆå§‹åŒ–å®Œæˆ: {config.run_name}")
    tqdm.write(f"ä¿å­˜ç›®å½•: {save_dir}")
    tqdm.write(f"è®¾å¤‡: {device}")

    # Meta-testè®¾ç½®
    meta_test_enabled = hasattr(config, "fsl_task") and config.fsl_task is not None
    if meta_test_enabled:
        tqdm.write(
            f"âœ“ Meta-test enabled with {config.fsl_task.N_way}-way {config.fsl_task.K_shot}-shot"
        )
    else:
        tqdm.write("âœ— Meta-test disabled: no fsl_task config")

    # æ—©åœæœºåˆ¶åˆå§‹åŒ–
    enable_early_stopping = getattr(config.vae.train, "enable_early_stopping", True)

    if enable_early_stopping:
        early_stop_patience = getattr(config.vae.train, "early_stop_patience", 5)
        early_stop_min_delta = getattr(config.vae.train, "early_stop_min_delta", 0.01)
        early_stopping = EarlyStopping(
            patience=early_stop_patience,
            min_delta=early_stop_min_delta,
            mode="max",  # Meta-Testå‡†ç¡®ç‡è¶Šé«˜è¶Šå¥½
        )
        tqdm.write(
            f"âœ“ Early stopping enabled: patience={early_stop_patience}, min_delta={early_stop_min_delta}"
        )
    else:
        early_stopping = None
        tqdm.write("âœ— Early stopping disabled - full training curve will be recorded")

    # åˆå§‹åŒ–å…ƒæµ‹è¯•ç›¸å…³ç»„ä»¶
    encoder = model.encoder
    if meta_test_enabled:
        _init_meta_test_components(config, device)

    # ä¸»è®­ç»ƒå¾ªç¯
    best_test_loss = float("inf")
    best_meta_test_acc = 0.0
    best_checkpoint_path = None

    progress_bar = tqdm(
        range(config.vae.train.num_epochs),
        desc="Training",
        ncols=100,
        leave=True,
        ascii=True,
    )

    for epoch in progress_bar:
        # è®­ç»ƒé˜¶æ®µ
        train_losses = _train_epoch(model, train_loader, optimizer, config, device)
        mean_train_loss = np.mean(train_losses["total"])

        # æäº¤è®­ç»ƒæŸå¤±åˆ°wandb
        train_log = {
            "epoch": epoch,
            "train_loss": mean_train_loss,
            "train_rec_loss": np.mean(train_losses["rec"]),
            "train_kl_loss": np.mean(train_losses["kl"]),
            "train_edge_loss": np.mean(train_losses["edge"]),
            "lr": optimizer.param_groups[0]["lr"],
        }
        wandb.log(train_log)

        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler:
            if use_warmup and epoch < getattr(config.vae.train, "warmup_epochs", 0):
                warmup_scheduler.step()
            else:
                scheduler.step()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæµ‹è¯•
        should_test = (epoch % config.vae.train.test_interval == 0) or (
            epoch == config.vae.train.num_epochs - 1
        )

        if should_test:
            # æµ‹è¯•é˜¶æ®µ
            test_losses = _test_epoch(model, test_loader, config, device)
            mean_test_loss = np.mean(test_losses["total"])

            # å…ƒæµ‹è¯•è¯„ä¼°
            meta_test_acc = 0.0
            if meta_test_enabled:
                meta_test_acc = _meta_test_evaluation(encoder, dataset, config, device, epoch)

            # æäº¤æµ‹è¯•æŸå¤±å’ŒæŒ‡æ ‡åˆ°wandb
            test_log = {
                "epoch": epoch,
                "test_loss": mean_test_loss,
                "test_rec_loss": np.mean(test_losses["rec"]),
                "test_kl_loss": np.mean(test_losses["kl"]),
                "test_edge_loss": np.mean(test_losses["edge"]),
                "meta_test_accuracy": meta_test_acc,
            }
            wandb.log(test_log)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best_loss = mean_test_loss < best_test_loss
            is_best_meta_acc = meta_test_acc > best_meta_test_acc

            if is_best_loss:
                best_test_loss = mean_test_loss
                checkpoint_path = _save_checkpoint(
                    model,
                    optimizer,
                    scheduler,
                    epoch,
                    mean_test_loss,
                    meta_test_acc,
                    save_dir,
                    "best_loss",
                    config,
                )
                progress_bar.write(f"âœ“ New best loss: {mean_test_loss:.6f}")

            if is_best_meta_acc:
                best_meta_test_acc = meta_test_acc
                best_checkpoint_path = _save_checkpoint(
                    model,
                    optimizer,
                    scheduler,
                    epoch,
                    mean_test_loss,
                    meta_test_acc,
                    save_dir,
                    "best_meta_acc",
                    config,
                )
                progress_bar.write(f"âœ“ New best meta-acc: {meta_test_acc:.4f}")

            # æ—©åœæ£€æŸ¥
            if enable_early_stopping and meta_test_enabled and early_stopping is not None:
                should_early_stop = early_stopping(meta_test_acc)
                if should_early_stop:
                    progress_bar.write(f"ğŸ›‘ Early stopping triggered at epoch {epoch}")
                    progress_bar.write(f"   Best Meta-Test Acc: {early_stopping.best_score:.4f}")
                    progress_bar.write(
                        f"   No improvement for {early_stopping.patience} consecutive evaluations"
                    )
                    _save_checkpoint(
                        model,
                        optimizer,
                        scheduler,
                        epoch,
                        mean_test_loss,
                        meta_test_acc,
                        save_dir,
                        "early_stop",
                        config,
                    )
                    break

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix(
                {
                    "Train": f"{mean_train_loss:.6f}",
                    "Best-Meta": f"{best_meta_test_acc:.4f}",
                }
            )

            tqdm.write(
                f"Epoch {epoch}: Train={mean_train_loss:.6f}, Test={mean_test_loss:.6f}, Meta-Test Acc={meta_test_acc:.4f}"
            )
        else:
            # éæµ‹è¯•epoch
            progress_bar.set_postfix(
                {
                    "Train": f"{mean_train_loss:.6f}",
                    "Best-Meta": f"{best_meta_test_acc:.4f}",
                }
            )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_test_losses = _test_epoch(model, test_loader, config, device)
    final_mean_test_loss = np.mean(final_test_losses["total"])
    final_meta_test_acc = 0.0
    if meta_test_enabled:
        final_meta_test_acc = _meta_test_evaluation(
            encoder, dataset, config, device, config.vae.train.num_epochs - 1
        )

    final_checkpoint_path = _save_checkpoint(
        model,
        optimizer,
        scheduler,
        config.vae.train.num_epochs - 1,
        final_mean_test_loss,
        final_meta_test_acc,
        save_dir,
        "final",
        config,
    )

    tqdm.write(
        f"Training completed. Best test loss: {best_test_loss:.6f}, Best meta-test acc: {best_meta_test_acc:.4f}"
    )

    # å¦‚æœæ²¡æœ‰æœ€ä½³metaå‡†ç¡®ç‡æ£€æŸ¥ç‚¹ï¼Œåˆ™ä½¿ç”¨æœ€ç»ˆæ£€æŸ¥ç‚¹
    if best_checkpoint_path is None:
        best_checkpoint_path = final_checkpoint_path

    return {
        "save_dir": save_dir,
        "best_checkpoint": best_checkpoint_path,
        "final_checkpoint": final_checkpoint_path,
        "best_test_loss": best_test_loss,
        "best_meta_test_acc": best_meta_test_acc,
    }


def _init_wandb(config):
    """åˆå§‹åŒ–wandb"""
    mode = "disabled" if config.debug else ("online" if config.wandb.online else "offline")

    # ä»é…ç½®ä¸­è·å– wandb è¾“å‡ºç›®å½•
    wandb_output_dir = getattr(config.wandb, "output_dir", "logs")
    wandb_dir = os.path.join(wandb_output_dir, "wandb")

    wandb.init(
        project=config.wandb.project,
        entity=config.wandb.entity,
        name=config.run_name,
        config=OmegaConf.to_container(config, resolve=True),
        mode=mode,
        dir=wandb_dir,
    )


def _init_model(config, device):
    """åˆå§‹åŒ–VAEæ¨¡å‹"""
    # åˆ›å»ºGraphVAEé…ç½®å¯¹è±¡
    from types import SimpleNamespace

    vae_config = SimpleNamespace()
    vae_config.pred_node_class = config.vae.loss.pred_node_class
    vae_config.pred_edge = config.vae.loss.pred_edge
    vae_config.pred_graph_class = config.vae.loss.pred_graph_class
    vae_config.use_kl_loss = config.vae.loss.use_kl_loss
    vae_config.use_base_proto_loss = config.vae.loss.use_base_proto_loss
    vae_config.use_sep_proto_loss = config.vae.loss.use_sep_proto_loss

    # è®¾ç½®ç¼–ç å™¨å’Œè§£ç å™¨é…ç½®
    vae_config.encoder_config = config.vae.encoder
    vae_config.encoder_config.input_feature_dim = config.data.max_feat_num

    vae_config.decoder_config = config.vae.decoder
    vae_config.decoder_config.latent_feature_dim = config.vae.encoder.latent_feature_dim
    vae_config.decoder_config.output_feature_dim = config.data.max_feat_num

    vae_config.latent_dim = config.vae.encoder.latent_feature_dim
    vae_config.device = device

    # åˆ›å»ºGraphVAE
    model = GraphVAE(vae_config).to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        model.parameters(),
        lr=config.vae.train.lr,
        weight_decay=config.vae.train.weight_decay,
    )

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    warmup_scheduler = None
    use_warmup = False

    if config.vae.train.lr_schedule:
        scheduler_type = getattr(config.vae.train, "scheduler_type", "exponential")

        if scheduler_type == "exponential":
            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.vae.train.lr_decay)
        elif scheduler_type == "cosine":
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=config.vae.train.num_epochs,
                eta_min=config.vae.train.lr * 0.01,
            )
        elif scheduler_type == "step":
            step_size = getattr(config.vae.train, "lr_step_size", 100)
            scheduler = optim.lr_scheduler.StepLR(
                optimizer, step_size=step_size, gamma=config.vae.train.lr_decay
            )
        else:
            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.vae.train.lr_decay)

        # Warm-upè°ƒåº¦å™¨æ”¯æŒ
        warmup_epochs = getattr(config.vae.train, "warmup_epochs", 0)
        if warmup_epochs > 0:
            from torch.optim.lr_scheduler import LambdaLR

            def warmup_lambda(epoch):
                if epoch < warmup_epochs:
                    return (epoch + 1) / warmup_epochs
                return 1.0

            warmup_scheduler = LambdaLR(optimizer, warmup_lambda)
            use_warmup = True
            tqdm.write(f"âœ“ Warm-up enabled: {warmup_epochs} epochs")

        tqdm.write(f"âœ“ LR Scheduler: {scheduler_type}, decay: {config.vae.train.lr_decay}")

    tqdm.write(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    return model, optimizer, scheduler, warmup_scheduler, use_warmup


def _create_save_dir(config):
    """åˆ›å»ºä¿å­˜ç›®å½•"""
    if hasattr(config.paths, "vae_save_dir"):
        save_dir = config.paths.vae_save_dir
    else:
        save_dir = os.path.join(config.paths.save_dir, config.exp_name, config.timestamp)

    os.makedirs(save_dir, exist_ok=True)
    return save_dir


def _init_meta_test_components(config, device):
    """åˆå§‹åŒ–meta-testè¯„ä¼°ç»„ä»¶"""
    fsl_config = config.fsl_task
    N_way = fsl_config.N_way
    meta_test_tasks = getattr(fsl_config, "meta_test_tasks", 10)
    latent_dim = config.vae.encoder.latent_feature_dim

    print(f"âœ“ Meta-test components initialized:")
    print(f"  N-way: {N_way}, K-shot: {fsl_config.K_shot}, R-query: {fsl_config.R_query}")
    print(f"  Meta-test tasks: {meta_test_tasks}")
    print(f"  Latent dim: {latent_dim}")


def _train_epoch(model, train_loader, optimizer, config, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    losses = {"total": [], "rec": [], "kl": [], "edge": []}

    for batch in train_loader:
        x, adj, labels = load_batch(batch, device)

        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        (
            rec_loss,
            kl_loss,
            edge_loss,
            base_proto_loss,
            sep_proto_loss,
            graph_classification_loss,
            acc_proto,
        ) = model(x, adj, labels)

        # è®¡ç®—æ€»æŸå¤±
        total_loss = (
            config.vae.train.rec_weight * rec_loss
            + config.vae.train.kl_regularization * kl_loss
            + config.vae.train.edge_weight * edge_loss
            + config.vae.train.base_proto_weight * base_proto_loss
            + config.vae.train.sep_proto_weight * sep_proto_loss
            + config.vae.train.graph_classification_weight * graph_classification_loss
        )

        # åå‘ä¼ æ’­
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.vae.train.grad_norm)
        optimizer.step()

        # è®°å½•æŸå¤±
        losses["total"].append(total_loss.item())
        losses["rec"].append(rec_loss.item())
        losses["kl"].append(kl_loss.item())
        losses["edge"].append(edge_loss.item())

    return losses


def _test_epoch(model, test_loader, config, device):
    """æµ‹è¯•ä¸€ä¸ªepoch"""
    model.eval()
    losses = {"total": [], "rec": [], "kl": [], "edge": []}

    with torch.no_grad():
        for batch in test_loader:
            x, adj, labels = load_batch(batch, device)

            # å‰å‘ä¼ æ’­
            (
                rec_loss,
                kl_loss,
                edge_loss,
                base_proto_loss,
                sep_proto_loss,
                graph_classification_loss,
                acc_proto,
            ) = model(x, adj, labels)

            # è®¡ç®—æ€»æŸå¤±
            total_loss = (
                config.vae.train.rec_weight * rec_loss
                + config.vae.train.kl_regularization * kl_loss
                + config.vae.train.edge_weight * edge_loss
                + config.vae.train.base_proto_weight * base_proto_loss
                + config.vae.train.sep_proto_weight * sep_proto_loss
                + config.vae.train.graph_classification_weight * graph_classification_loss
            )

            # è®°å½•æŸå¤±
            losses["total"].append(total_loss.item())
            losses["rec"].append(rec_loss.item())
            losses["kl"].append(kl_loss.item())
            losses["edge"].append(edge_loss.item())

    return losses


def _meta_test_evaluation(encoder, dataset, config, device, epoch):
    """Meta-test evaluation using linear probing"""
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„FSLå‚æ•°
    N_way = config.fsl_task.N_way
    K_shot = config.fsl_task.K_shot
    R_query = config.fsl_task.R_query
    meta_test_tasks = getattr(config.fsl_task, "meta_test_tasks", 10)

    encoder.eval()
    all_task_accuracies = []

    try:
        successful_tasks = 0
        failed_tasks = 0
        
        for task_idx in range(meta_test_tasks):
            task = dataset.sample_one_task(
                is_train=False,
                N_way=N_way,
                K_shot=K_shot,
                R_query=R_query,
            )

            if task is None:
                failed_tasks += 1
                continue
            
            successful_tasks += 1

            # æå–ä»»åŠ¡æ•°æ®
            support_x = task["support_set"]["x"].to(device)
            support_adj = task["support_set"]["adj"].to(device)
            support_labels = task["support_set"]["label"].to(device)
            query_x = task["query_set"]["x"].to(device)
            query_adj = task["query_set"]["adj"].to(device)
            query_labels = task["query_set"]["label"].to(device)

            with torch.no_grad():
                # æå–ç‰¹å¾
                support_features = _extract_features(encoder, support_x, support_adj, device)
                query_features = _extract_features(encoder, query_x, query_adj, device)

                # ä½¿ç”¨åŸå‹ç½‘ç»œæµ‹è¯•
                accuracy = _test_with_prototypes(
                    encoder, support_features, support_labels, query_features, query_labels, N_way
                )
                all_task_accuracies.append(accuracy)

        if successful_tasks > 0:
            mean_accuracy = np.mean(all_task_accuracies)
            return mean_accuracy
        else:
            return 0.0

    except Exception as e:
        tqdm.write(f"Meta-test evaluation error: {e}")
        return 0.0


def _extract_features(encoder, x_batch, adj_batch, device):
    """ä½¿ç”¨ç¼–ç å™¨æå–ç‰¹å¾"""
    with torch.no_grad():
        # ç”Ÿæˆnode_mask
        node_mask = node_flags(adj_batch)
        node_mask = node_mask.unsqueeze(-1)  # å¢åŠ æœ€åä¸€ä¸ªç»´åº¦

        # ä½¿ç”¨ç¼–ç å™¨æå–ç‰¹å¾
        posterior = encoder(x_batch, adj_batch, node_mask)
        z_mu = posterior.mode()  # è·å–åéªŒåˆ†å¸ƒçš„æ¨¡å¼

        # å¯¹äºå›¾çº§åˆ«çš„åˆ†ç±»ï¼Œæˆ‘ä»¬éœ€è¦èšåˆèŠ‚ç‚¹ç‰¹å¾
        # ä½¿ç”¨å¹³å‡æ± åŒ–ï¼ŒåŒæ—¶è€ƒè™‘node_mask
        node_mask_for_pooling = node_mask.squeeze(-1)  # [batch_size, num_nodes]
        masked_features = z_mu * node_mask.expand_as(z_mu)  # åº”ç”¨mask

        # è®¡ç®—æ¯ä¸ªå›¾çš„æœ‰æ•ˆèŠ‚ç‚¹æ•°
        num_valid_nodes = node_mask_for_pooling.sum(dim=1, keepdim=True)  # [batch_size, 1]
        num_valid_nodes = torch.clamp(num_valid_nodes, min=1.0)  # é¿å…é™¤é›¶

        # å¹³å‡æ± åŒ–å¾—åˆ°å›¾çº§ç‰¹å¾
        graph_features = masked_features.sum(dim=1) / num_valid_nodes  # [batch_size, latent_dim]

        return graph_features


def _test_with_prototypes(
    encoder, support_features, support_labels, query_features, query_labels, n_way
):
    """ä½¿ç”¨åŸå‹ç½‘ç»œè¿›è¡Œåˆ†ç±»ï¼Œæ”¯æŒæ­£ç¡®çš„å‡ ä½•è·ç¦»è®¡ç®—"""
    with torch.no_grad():
        prototypes = []
        for c in range(n_way):
            # ç­›é€‰å‡ºå±äºç±»åˆ«cçš„æ ·æœ¬ç‰¹å¾
            class_features = support_features[support_labels == c]
            # è®¡ç®—åŸå‹ï¼ˆç±»åˆ«ç‰¹å¾çš„å‡å€¼ï¼‰
            if class_features.size(0) > 0:
                prototype = class_features.mean(dim=0)
                prototypes.append(prototype)

        # å¦‚æœæŸä¸ªç±»åˆ«åœ¨æ”¯æŒé›†ä¸­æ²¡æœ‰æ ·æœ¬ï¼Œåˆ™æ— æ³•è¯„ä¼°
        if len(prototypes) != n_way:
            return 0.0

        prototypes = torch.stack(prototypes)  # [n_way, latent_dim]

        # æ ¹æ®ç¼–ç å™¨ç±»å‹é€‰æ‹©æ­£ç¡®çš„è·ç¦»è®¡ç®—æ–¹å¼
        if hasattr(encoder, "manifold") and encoder.manifold is not None:
            # åŒæ›²æµå½¢ï¼šä½¿ç”¨åŒæ›²è·ç¦»
            manifold = encoder.manifold

            # æ‰©å±•ç»´åº¦ä»¥è®¡ç®—æ‰¹é‡è·ç¦»
            query_expanded = query_features.unsqueeze(1)  # [num_query, 1, latent_dim]
            prototypes_expanded = prototypes.unsqueeze(0)  # [1, n_way, latent_dim]

            # è®¡ç®—åŒæ›²è·ç¦»
            distances = manifold.dist(query_expanded, prototypes_expanded)  # [num_query, n_way]
            distances = distances.squeeze(-1) if distances.dim() > 2 else distances
        else:
            # æ¬§å‡ é‡Œå¾—ç©ºé—´ï¼šä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»
            distances = torch.sum(
                (query_features.unsqueeze(1) - prototypes.unsqueeze(0)) ** 2, dim=2
            )

        # é¢„æµ‹ç±»åˆ«ä¸ºè·ç¦»æœ€å°çš„åŸå‹å¯¹åº”çš„ç±»åˆ«
        predictions = torch.argmin(distances, dim=1)

        # è®¡ç®—å‡†ç¡®ç‡
        correct = (predictions == query_labels).float().sum().item()
        accuracy = correct / len(query_labels) if len(query_labels) > 0 else 0.0

    return accuracy


def _save_checkpoint(
    model, optimizer, scheduler, epoch, test_loss, meta_test_acc, save_dir, checkpoint_type, config
):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        "epoch": epoch,
        "model_config": OmegaConf.to_container(config, resolve=True),
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "test_loss": test_loss,
        "meta_test_acc": meta_test_acc,
    }

    if scheduler:
        checkpoint["scheduler_state_dict"] = scheduler.state_dict()

    # ä¿å­˜æŒ‡å®šç±»å‹çš„æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(save_dir, f"{checkpoint_type}.pth")
    torch.save(checkpoint, checkpoint_path)

    return checkpoint_path

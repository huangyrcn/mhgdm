# ScoreNetwork_X.py
# è¯¥æ–‡ä»¶åŒ…å«å¤šç§åˆ†æ•°ç½‘ç»œï¼ˆScore Networkï¼‰å®ç°ï¼Œæ”¯æŒæ¬§å¼ç©ºé—´ã€åŒæ›²ç©ºé—´ï¼ˆPoincare Ballï¼‰å’Œå¸¦æ³¨æ„åŠ›æœºåˆ¶çš„å›¾ç¥ç»ç½‘ç»œã€‚

import torch
import torch.nn.functional as F
from torch import nn

from utils.manifolds_utils import exp_after_transp0
from utils.graph_utils import mask_x, pow_tensor

from layers.hyp_layers import HGCLayer, HGATLayer
from layers.layers import GCLayer, GATLayer
from layers.layers import DenseGCNConv, MLP
from utils.model_utils import get_timestep_embedding
from layers.attention import AttentionLayer

# æ¬§å¼ç©ºé—´ä¸‹çš„åˆ†æ•°ç½‘ç»œï¼ŒåŸºäºå¤šå±‚GCNå®ç°
class ScoreNetworkX(torch.nn.Module):
    def __init__(self, max_feat_num, depth, nhid):
        super(ScoreNetworkX, self).__init__()
        self.nfeat = max_feat_num
        self.depth = depth
        self.nhid = nhid
        # å †å å¤šå±‚GCN
        self.layers = torch.nn.ModuleList()
        for _ in range(self.depth):
            if _ == 0:
                self.layers.append(DenseGCNConv(self.nfeat, self.nhid))
            else:
                self.layers.append(DenseGCNConv(self.nhid, self.nhid))
        # æ‹¼æ¥æ‰€æœ‰å±‚è¾“å‡ºåï¼Œæœ€ç»ˆMLPè¾“å‡ºåˆ†æ•°
        self.fdim = self.nfeat + self.depth * self.nhid
        self.final = MLP(num_layers=3, input_dim=self.fdim, hidden_dim=2*self.fdim, output_dim=self.nfeat, 
                            use_bn=False, activate_func=F.elu)
        self.activation = torch.tanh

    def forward(self, x, adj, flags, t):
        # å‰å‘ä¼ æ’­ï¼šå¤šå±‚GCN+æ¿€æ´»ï¼Œæ‹¼æ¥ç‰¹å¾ï¼ŒMLPè¾“å‡ºåˆ†æ•°
        x_list = [x]
        for _ in range(self.depth):
            x = self.layers[_](x, adj)
            x = self.activation(x)
            x_list.append(x)
        xs = torch.cat(x_list, dim=-1) # B x N x (F + num_layers x H)
        out_shape = (adj.shape[0], adj.shape[1], -1)
        x = self.final(xs).view(*out_shape)
        x = mask_x(x, flags)
        return x


# GMH: Graph Multi-Head Attention åˆ†æ•°ç½‘ç»œï¼Œæ”¯æŒå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
class ScoreNetworkX_GMH(torch.nn.Module):
    def __init__(self, max_feat_num, depth, nhid, num_linears,
                 c_init, c_hid, c_final, adim, num_heads=4, conv='GCN'):
        super().__init__()
        self.depth = depth
        self.c_init = c_init
        # å¤šå±‚AttentionLayerå †å 
        self.layers = torch.nn.ModuleList()
        for _ in range(self.depth):
            if _ == 0:
                self.layers.append(AttentionLayer(num_linears, max_feat_num, nhid, nhid, c_init, 
                                                  c_hid, num_heads, conv))
            elif _ == self.depth - 1:
                self.layers.append(AttentionLayer(num_linears, nhid, adim, nhid, c_hid, 
                                                  c_final, num_heads, conv))
            else:
                self.layers.append(AttentionLayer(num_linears, nhid, adim, nhid, c_hid, 
                                                  c_hid, num_heads, conv))
        fdim = max_feat_num + depth * nhid
        self.final = MLP(num_layers=3, input_dim=fdim, hidden_dim=2*fdim, output_dim=max_feat_num, 
                         use_bn=False, activate_func=F.elu)
        self.activation = torch.tanh

    def forward(self, x, adj, flags):
        # å‰å‘ä¼ æ’­ï¼šå¤šå±‚å¤šå¤´æ³¨æ„åŠ›+æ¿€æ´»+ç‰¹å¾æ‹¼æ¥+MLPè¾“å‡º
        adjc = pow_tensor(adj, self.c_init)
        x_list = [x]
        for _ in range(self.depth):
            x, adjc = self.layers[_](x, adjc, flags)
            x = self.activation(x)
            x_list.append(x)
        xs = torch.cat(x_list, dim=-1) # B x N x (F + num_layers x H)
        out_shape = (adj.shape[0], adj.shape[1], -1)
        x = self.final(xs).view(*out_shape)
        x = mask_x(x, flags)
        return x
# åŒæ›²ç©ºé—´ï¼ˆPoincare Ballç­‰ï¼‰ä¸‹çš„åˆ†æ•°ç½‘ç»œï¼Œæ”¯æŒHGAT/HGCNç­‰å¤šç§å±‚

class ScoreNetworkX_poincare(torch.nn.Module):
    def __init__(self, max_feat_num, depth, nhid,manifold,edge_dim,GCN_type,**kwargs):
        super(ScoreNetworkX_poincare, self).__init__()
        self.manifold = manifold
        self.nfeat = max_feat_num
        self.depth = depth
        self.nhid = nhid
        # æ ¹æ®GCN_typeé€‰æ‹©å±‚ç±»å‹ï¼ˆæ”¯æŒæ¬§å¼å’ŒåŒæ›²ï¼‰
        if GCN_type == 'GCN':
            layer_type = GCLayer
        elif GCN_type == 'GAT':
            layer_type = GATLayer
        elif GCN_type == 'HGCN':
            layer_type = HGCLayer
        elif GCN_type == 'HGAT':
            layer_type = HGATLayer
        else:
            raise AttributeError
        self.layers = torch.nn.ModuleList()
        if self.manifold is not None:
            # manifoldåˆ—è¡¨ï¼Œæ”¯æŒå¤šå±‚åŒæ›²ç©ºé—´ç‰¹å¾ä¼ é€’
            self.manifolds = [self.manifold]*(depth+1)
            for i in range(self.depth):
                if i == 0:
                    self.layers.append(layer_type(self.nfeat, self.nhid,self.manifolds[i],self.manifolds[i+1],edge_dim=edge_dim))
                else:
                    self.layers.append(layer_type(self.nhid, self.nhid,self.manifolds[i],self.manifolds[i+1],edge_dim=edge_dim))
        else:
            for i in range(self.depth):
                if i == 0:
                    self.layers.append(layer_type(self.nfeat, self.nhid,edge_dim=edge_dim))
                else:
                    self.layers.append(layer_type(self.nhid, self.nhid,edge_dim=edge_dim))
        self.fdim = self.nfeat + self.depth * self.nhid
        self.final = MLP(num_layers=3, input_dim=self.fdim, hidden_dim=2*self.fdim, output_dim=self.nfeat,
                            use_bn=False, activate_func=F.elu)
        # æ—¶é—´åµŒå…¥å’Œç¼©æ”¾ï¼Œç”¨äºæ‰©æ•£æ¨¡å‹çš„æ—¶é—´è°ƒåˆ¶
        self.temb_net = MLP(num_layers=3, input_dim=self.nfeat, hidden_dim=2*self.nfeat, output_dim=self.nfeat,
                            use_bn=False, activate_func=F.elu)
        self.time_scale = nn.Sequential(
            nn.Linear(self.nfeat+1, self.nfeat),
            nn.ReLU(),
            nn.Linear(self.nfeat, 1)
        )

    def forward(self, x, adj, flags, t):
        # Check and adapt input dimension if necessary
        if x.size(-1) != self.nfeat:
            if x.size(-1) < self.nfeat:
                padding_size = self.nfeat - x.size(-1)
                padding = torch.zeros(*x.shape[:-1], padding_size, device=x.device, dtype=x.dtype)
                x = torch.cat([x, padding], dim=-1)
                print(f"Warning: Padded input x feature dim from {x.size(-1)-padding_size} to {self.nfeat}")
            else: # x.size(-1) > self.nfeat
                # Truncate or raise error - Truncating for now
                x = x[..., :self.nfeat]
                print(f"Warning: Truncated input x feature dim from {x.size(-1)} to {self.nfeat}")

        # å‰å‘ä¼ æ’­ï¼šåŒæ›²ç©ºé—´ç‰¹å¾å˜æ¢+å¤šå±‚HGAT/HGCN+æ—¶é—´è°ƒåˆ¶
        xt = x.clone()
        temb = get_timestep_embedding(t, self.nfeat)
        # Note: temb_net input dim is self.nfeat, output dim is self.nfeat
        temb_output = self.temb_net(temb)
        
        # Ensure temb_output is correctly shaped for broadcasting if needed by exp_after_transp0
        # Assuming exp_after_transp0 handles broadcasting or expects [B, N, D]
        # If temb_output is [B, D], it might need repeating: temb_output.unsqueeze(1).repeat(1, x.size(1), 1)
        # However, let's assume geoopt handles the [B, D] case for v_at_0 in transp0

        x = exp_after_transp0(x, temb_output, self.manifolds[0])
        if self.manifold is not None:
            x_list = [self.manifolds[0].logmap0(x)]
        else:
            x_list = [x]
        for i in range(self.depth):
            x = self.layers[i]((x, adj))[0]
            if self.manifold is not None:
                x_list.append(self.manifolds[i+1].logmap0(x))
            else:
                x_list.append(x)
        xs = torch.cat(x_list, dim=-1) # B x N x (F + num_layers x H)
        out_shape = (adj.shape[0], adj.shape[1], -1)
        x = self.final(xs).view(*out_shape)
        # expmap0/logmapç­‰æ“ä½œä¿è¯è¾“å‡ºåœ¨åˆ‡ç©ºé—´
        x = self.manifold.expmap0(x)
        x = self.manifold.logmap(xt, x)
        # æ—¶é—´ç¼©æ”¾ï¼ŒåŒ…å«conformal factor
        x = x * self.time_scale(torch.cat([temb.repeat(1,x.size(1),1),self.manifold.lambda_x(xt,keepdim=True)],dim=-1))
        x = mask_x(x, flags)
        return x

class ScoreNetworkX_poincare_proto(torch.nn.Module):
    def __init__(self, max_feat_num, depth, nhid,manifold,edge_dim,GCN_type,**kwargs):
        super().__init__()
        self.manifold = manifold
        self.nfeat = max_feat_num
        self.depth = depth
        self.nhid = nhid
        # æ ¹æ®GCN_typeé€‰æ‹©å±‚ç±»å‹ï¼ˆæ”¯æŒæ¬§å¼å’ŒåŒæ›²ï¼‰
        if GCN_type == 'GCN':
            layer_type = GCLayer
        elif GCN_type == 'GAT':
            layer_type = GATLayer
        elif GCN_type == 'HGCN':
            layer_type = HGCLayer
        elif GCN_type == 'HGAT':
            layer_type = HGATLayer
        else:
            raise AttributeError
        self.layers = torch.nn.ModuleList()
        if self.manifold is not None:
            # manifoldåˆ—è¡¨ï¼Œæ”¯æŒå¤šå±‚åŒæ›²ç©ºé—´ç‰¹å¾ä¼ é€’
            self.manifolds = [self.manifold]*(depth+1)
            for i in range(self.depth):
                if i == 0:
                    self.layers.append(layer_type(self.nfeat, self.nhid,self.manifolds[i],self.manifolds[i+1],edge_dim=edge_dim))
                else:
                    self.layers.append(layer_type(self.nhid, self.nhid,self.manifolds[i],self.manifolds[i+1],edge_dim=edge_dim))
        else:
            for i in range(self.depth):
                if i == 0:
                    self.layers.append(layer_type(self.nfeat, self.nhid,edge_dim=edge_dim))
                else:
                    self.layers.append(layer_type(self.nhid, self.nhid,edge_dim=edge_dim))
        self.fdim = self.nfeat + self.depth * self.nhid
        self.final = MLP(num_layers=3, input_dim=self.fdim, hidden_dim=2*self.fdim, output_dim=self.nfeat,
                            use_bn=False, activate_func=F.elu)
        # æ—¶é—´åµŒå…¥å’Œç¼©æ”¾ï¼Œç”¨äºæ‰©æ•£æ¨¡å‹çš„æ—¶é—´è°ƒåˆ¶
        self.temb_net = MLP(num_layers=3, input_dim=self.nfeat, hidden_dim=2*self.nfeat, output_dim=self.nfeat,
                            use_bn=False, activate_func=F.elu)
        self.time_scale = nn.Sequential(
            nn.Linear(self.nfeat+1, self.nfeat),
            nn.ReLU(),
            nn.Linear(self.nfeat, 1)
        )

    def forward(self, x, adj, flags, t,protos):
        # å‰å‘ä¼ æ’­ï¼šåŒæ›²ç©ºé—´ç‰¹å¾å˜æ¢+å¤šå±‚HGAT/HGCN+æ—¶é—´è°ƒåˆ¶
        xt = x.clone()
        temb = get_timestep_embedding(t, self.nfeat)
        x = exp_after_transp0(x,self.temb_net(temb),self.manifolds[0])
        if self.manifold is not None:
            x_list = [self.manifolds[0].logmap0(x)]
        else:
            x_list = [x]
        for i in range(self.depth):
            x = self.layers[i]((x, adj))[0]
            if self.manifold is not None:
                x_list.append(self.manifolds[i+1].logmap0(x))
            else:
                x_list.append(x)
        xs = torch.cat(x_list, dim=-1) # B x N x (F + num_layers x H)
        out_shape = (adj.shape[0], adj.shape[1], -1)
        x = self.final(xs).view(*out_shape)
        # expmap0/logmapç­‰æ“ä½œä¿è¯è¾“å‡ºåœ¨åˆ‡ç©ºé—´
        x = self.manifold.expmap0(x)
        x = self.manifold.logmap(xt, x)

            # ğŸ’¡ èå…¥åŸå‹ç›¸ä¼¼æ€§å¼•å¯¼
        if protos is not None:
            proto_dist = torch.cdist(xt, protos)       # [B, N, C]
            proto_sim = -proto_dist                    # è¶Šå¤§è¶Šç›¸ä¼¼
            proto_context = torch.matmul(
                torch.softmax(proto_sim, dim=-1), protos
            )                                          # [B, N, d]
            proto_diff = self.manifold.logmap0(proto_context)  # æ˜ å°„åˆ°åˆ‡ç©ºé—´
            x = x + self.proto_weight * proto_diff     # åŠ æƒå åŠ è¯­ä¹‰å¼•å¯¼

        # æ—¶é—´ç¼©æ”¾ï¼ŒåŒ…å«conformal factor

        time_input = torch.cat([temb.repeat(1, x.size(1), 1), self.manifold.lambda_x(xt, keepdim=True)], dim=-1)
        x = x * self.time_scale(time_input)
        x = mask_x(x, flags)
        
        return x

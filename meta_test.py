"""
Meta-testè¯„ä¼°å‡½æ•°
æ”¯æŒä½¿ç”¨encoderæˆ–encoder+åˆ†æ•°ç½‘ç»œè¿›è¡Œfew-shotå­¦ä¹ è¯„ä¼°
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import wandb
from tqdm import tqdm
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from omegaconf import OmegaConf

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.config_utils import load_config
from utils.data_utils import MyDataset
from utils.loader import load_seed, load_device
from utils.graph_utils import node_flags
from models.GraphVAE import GraphVAE
from models.Decoders import Classifier


def run_meta_test(config, use_augmentation=False, checkpoint_paths=None):
    """
    è¿è¡ŒMeta-testè¯„ä¼°

    Args:
        config: é…ç½®å¯¹è±¡
        use_augmentation: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼ºï¼ˆåˆ†æ•°ç½‘ç»œï¼‰
        checkpoint_paths: æ£€æŸ¥ç‚¹è·¯å¾„å­—å…¸ï¼ŒåŒ…å«:
            - vae_checkpoint: VAEæ£€æŸ¥ç‚¹è·¯å¾„
            - score_checkpoint: Scoreæ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆä»…åœ¨use_augmentation=Trueæ—¶éœ€è¦ï¼‰

    Returns:
        dict: è¯„ä¼°ç»“æœï¼ŒåŒ…å«accuracy, f1ç­‰æŒ‡æ ‡
    """
    if checkpoint_paths is None or "vae_checkpoint" not in checkpoint_paths:
        raise ValueError("å¿…é¡»æä¾›VAEæ£€æŸ¥ç‚¹è·¯å¾„")

    vae_checkpoint_path = checkpoint_paths["vae_checkpoint"]

    # ç¡®ä¿wandbä¼šè¯å¹²å‡€
    try:
        if wandb.run is not None:
            wandb.finish()
            print("âœ“ å·²å…³é—­ä¹‹å‰çš„wandbä¼šè¯")
    except:
        pass

    # åŸºç¡€è®¾ç½®
    device = load_device(config)
    load_seed(config.seed)

    # åˆå§‹åŒ–wandb
    wandb_suffix = "_aug" if use_augmentation else "_no_aug"
    mode = (
        "disabled"
        if getattr(config, "debug", False)
        else ("online" if config.wandb.online else "offline")
    )

    wandb.init(
        project=f"{config.wandb.project}_Meta",
        entity=config.wandb.entity,
        name=f"{config.run_name}_meta{wandb_suffix}",
        config=OmegaConf.to_container(config, resolve=True),
        mode=mode,
    )

    # åŠ è½½æ•°æ®é›†
    dataset = MyDataset(config.data, config.fsl_task)
    train_loader, test_loader = dataset.get_loaders()

    # åŠ è½½ç¼–ç å™¨
    encoder = _load_encoder(vae_checkpoint_path, device)

    # å¦‚æœä½¿ç”¨å¢å¼ºï¼Œè¿˜éœ€è¦åŠ è½½åˆ†æ•°ç½‘ç»œ
    diffusion_model = None
    if use_augmentation and "score_checkpoint" in checkpoint_paths:
        score_checkpoint_path = checkpoint_paths["score_checkpoint"]
        try:
            diffusion_model = _load_diffusion_model(score_checkpoint_path, config, device)
            if diffusion_model is not None:
                print(f"âœ“ åˆ†æ•°ç½‘ç»œå·²åŠ è½½ï¼Œå¯ç”¨æ•°æ®å¢å¼º")
            else:
                print(f"âš ï¸ åˆ†æ•°ç½‘ç»œåŠ è½½è¿”å›Noneï¼Œå°†ä¸ä½¿ç”¨å¢å¼º")
        except Exception as e:
            print(f"âš ï¸ åˆ†æ•°ç½‘ç»œåŠ è½½å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨å¢å¼º: {e}")
            diffusion_model = None

    print(f"Meta-test åˆå§‹åŒ–å®Œæˆ")
    print(f"Device: {device}")
    print(f"æ•°æ®å¢å¼º: {'å¯ç”¨' if diffusion_model is not None else 'ç¦ç”¨'}")

    # è¿è¡Œè¯„ä¼°
    results = _run_evaluation(
        dataset=dataset,
        encoder=encoder,
        diffusion_model=diffusion_model,
        config=config,
        device=device,
        use_augmentation=use_augmentation and diffusion_model is not None,
    )

    wandb.finish()
    return results


def _load_encoder(checkpoint_path, device):
    """åŠ è½½ç¼–ç å™¨"""
    print(f"Loading encoder from: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    vae_config = checkpoint["model_config"]

    # æ„å»ºVAEé…ç½®
    from types import SimpleNamespace

    model_config = SimpleNamespace()

    if "vae" in vae_config:
        # æ—§æ ¼å¼
        model_config.encoder_config = SimpleNamespace(**vae_config["vae"]["encoder"])
        model_config.decoder_config = SimpleNamespace(**vae_config["vae"]["decoder"])
        model_config.pred_node_class = vae_config["vae"]["loss"]["pred_node_class"]
        model_config.pred_edge = vae_config["vae"]["loss"]["pred_edge"]
        model_config.pred_graph_class = vae_config["vae"]["loss"]["pred_graph_class"]
        model_config.use_kl_loss = vae_config["vae"]["loss"]["use_kl_loss"]
        model_config.latent_dim = vae_config["vae"]["encoder"]["latent_feature_dim"]
    else:
        # æ–°æ ¼å¼
        model_config.encoder_config = SimpleNamespace(**vae_config["encoder"])
        model_config.decoder_config = SimpleNamespace(**vae_config["decoder"])
        model_config.pred_node_class = vae_config["loss"]["pred_node_class"]
        model_config.pred_edge = vae_config["loss"]["pred_edge"]
        model_config.pred_graph_class = vae_config["loss"]["pred_graph_class"]
        model_config.use_kl_loss = vae_config["loss"]["use_kl_loss"]
        model_config.latent_dim = vae_config["encoder"]["latent_feature_dim"]

    model_config.use_base_proto_loss = False
    model_config.use_sep_proto_loss = False
    model_config.device = device

    # åˆ›å»ºå¹¶åŠ è½½VAE
    vae_model = GraphVAE(model_config)
    vae_model.load_state_dict(checkpoint["model_state_dict"])
    vae_model.to(device)
    vae_model.eval()

    # æå–ç¼–ç å™¨
    encoder = vae_model.encoder
    encoder.requires_grad_(False)

    print("âœ“ Encoder loaded")
    return encoder


def _load_diffusion_model(checkpoint_path, config, device):
    """åŠ è½½åˆ†æ•°ç½‘ç»œæ¨¡å‹ï¼Œæ”¯æŒControlNetæ¶æ„"""
    print(f"Loading diffusion model from: {checkpoint_path}")

    try:
        # åŠ è½½checkpoint
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

        # ä»checkpointä¸­è·å–æ¨¡å‹é…ç½®
        if "model_config" in checkpoint:
            score_config = checkpoint["model_config"]
        else:
            # å¦‚æœæ²¡æœ‰ä¿å­˜é…ç½®ï¼Œä½¿ç”¨å½“å‰configä¸­çš„é…ç½®
            score_config = config.score

        # å¯¼å…¥åˆ†æ•°ç½‘ç»œæ¨¡å‹
        from models.ScoreNetwork_X import ScoreNetworkX_poincare
        from models.ScoreNetwork_A import ScoreNetworkA_poincare
        from models.ControlNet_Graph import GraphControlNet, create_graph_controlnet
        from utils.manifolds_utils import get_manifold

        # åˆ›å»ºmanifold
        manifold = get_manifold("PoincareBall", c=1.0)

        # ä»é…ç½®ä¸­è·å–ç½‘ç»œå‚æ•°ï¼Œæä¾›é»˜è®¤å€¼
        if isinstance(score_config, dict):
            # å¤„ç†å­—å…¸ç±»å‹çš„é…ç½®
            x_config = score_config.get("x", {})
            a_config = score_config.get("a", {})
        else:
            # å¤„ç†å¯¹è±¡ç±»å‹çš„é…ç½®
            x_config = getattr(score_config, "x", {}) if hasattr(score_config, "x") else {}
            a_config = getattr(score_config, "a", {}) if hasattr(score_config, "a") else {}

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not x_config:
            x_config = {"max_feat_num": config.data.get("max_feat_num", 16), "depth": 3, "nhid": 32}
        if not a_config:
            a_config = {
                "nhid": 32,
                "num_layers": 3,
                "num_linears": 2,
                "c_init": 2,
                "c_hid": 8,
                "c_final": 4,
                "adim": 32,
                "num_heads": 4,
            }

        # åˆ›å»ºåŸå§‹Xç½‘ç»œï¼ˆèŠ‚ç‚¹ç‰¹å¾åˆ†æ•°ç½‘ç»œï¼‰
        original_score_x = ScoreNetworkX_poincare(
            max_feat_num=x_config.get("max_feat_num", 16),
            depth=x_config.get("depth", 3),
            nhid=x_config.get("nhid", 32),
            manifold=manifold,
            edge_dim=1,
            GCN_type="HGCN",
        )

        # åˆ›å»ºåŸå§‹Aç½‘ç»œï¼ˆé‚»æ¥çŸ©é˜µåˆ†æ•°ç½‘ç»œï¼‰
        original_score_adj = ScoreNetworkA_poincare(
            max_feat_num=x_config.get("max_feat_num", 16),  # æ·»åŠ ç¼ºå¤±å‚æ•°
            max_node_num=config.data.max_node_num,  # ä¿®å¤ï¼šä½¿ç”¨dataè€Œä¸æ˜¯dataset
            nhid=a_config.get("nhid", 32),
            num_layers=a_config.get("num_layers", 3),
            num_linears=a_config.get("num_linears", 2),
            c_init=a_config.get("c_init", 2),
            c_hid=a_config.get("c_hid", 8),
            c_final=a_config.get("c_final", 4),
            adim=a_config.get("adim", 32),
            num_heads=a_config.get("num_heads", 4),
            conv="GCN",
            manifold=manifold,  # æ·»åŠ manifoldå‚æ•°
        )

        # åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ°åŸå§‹ç½‘ç»œ
        if "score_x_state_dict" in checkpoint:
            try:
                original_score_x.load_state_dict(checkpoint["score_x_state_dict"])
                print("âœ“ ScoreXç½‘ç»œæƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ScoreXç½‘ç»œæƒé‡åŠ è½½å¤±è´¥: {e}")

        if "score_adj_state_dict" in checkpoint:
            try:
                original_score_adj.load_state_dict(checkpoint["score_adj_state_dict"])
                print("âœ“ ScoreAdjç½‘ç»œæƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ScoreAdjç½‘ç»œæƒé‡åŠ è½½å¤±è´¥: {e}")

        # æ£€æŸ¥checkpointä¸­æœ‰ä»€ä¹ˆkeys
        print(f"ğŸ“‹ CheckpointåŒ…å«çš„keys: {list(checkpoint.keys())}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ControlNetæ¨¡å¼
        use_controlnet = config.get("use_controlnet", True)

        if use_controlnet:
            print("ğŸ¯ ä½¿ç”¨ControlNetæ¶æ„è¿›è¡Œç²¾ç¡®æ§åˆ¶ç”Ÿæˆ")
            # åˆ›å»ºControlNet
            controlnet = create_graph_controlnet(original_score_x, original_score_adj)

            # å¦‚æœæœ‰ControlNetæƒé‡ï¼ŒåŠ è½½å®ƒä»¬
            if "controlnet_state_dict" in checkpoint:
                controlnet.load_state_dict(checkpoint["controlnet_state_dict"])
                print("âœ“ ControlNetæƒé‡å·²åŠ è½½")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ControlNetæƒé‡ï¼Œå°†ä½¿ç”¨é›¶åˆå§‹åŒ–")

            controlnet.to(device)
            controlnet.eval()

            return {
                "type": "controlnet",
                "model": controlnet,
                "original_score_x": original_score_x,
                "original_score_adj": original_score_adj,
                "manifold": manifold,
                "device": device,
            }
        else:
            print("ğŸ”§ ä½¿ç”¨ä¼ ç»ŸScoreç½‘ç»œè¿›è¡Œæ•°æ®å¢å¼º")
            # ä¼ ç»Ÿæ–¹å¼
            original_score_x.to(device)
            original_score_adj.to(device)
            original_score_x.eval()
            original_score_adj.eval()

            return {
                "type": "traditional",
                "score_x": original_score_x,
                "score_adj": original_score_adj,
                "manifold": manifold,
                "device": device,
            }

    except Exception as e:
        print(f"âŒ åˆ†æ•°ç½‘ç»œåŠ è½½å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return None


def _get_embeddings(encoder, x, adj, device):
    """è·å–å›¾åµŒå…¥"""
    from utils.graph_utils import node_flags

    mask = node_flags(adj).unsqueeze(-1)

    with torch.no_grad():
        # ä½¿ç”¨ç¼–ç å™¨æå–ç‰¹å¾
        posterior = encoder(x, adj, mask)

        # å¤„ç†åˆ†å¸ƒè¾“å‡º - è·å–å‡å€¼æˆ–æ¨¡å¼
        if hasattr(posterior, "mode"):
            z = posterior.mode()
        elif hasattr(posterior, "mean"):
            z = posterior.mean
        else:
            z = posterior

        # ç¡®ä¿zæ˜¯èŠ‚ç‚¹çº§åˆ«çš„ç‰¹å¾ [batch_size, num_nodes, feature_dim]
        print(f"  ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {z.shape}")
        print(f"  maskå½¢çŠ¶: {mask.shape}")

        # æ£€æŸ¥æ˜¯å¦åœ¨åŒæ›²æµå½¢ä¸Š
        print(f"  encoder.manifoldå­˜åœ¨: {hasattr(encoder, 'manifold')}")
        if hasattr(encoder, "manifold"):
            print(f"  encoder.manifold: {encoder.manifold}")

        if hasattr(encoder, "manifold") and encoder.manifold is not None:
            # åŒæ›²ç©ºé—´ï¼šä½¿ç”¨æµå½¢ä¸Šçš„å¹³å‡æ± åŒ–
            print(f"  ä½¿ç”¨åŒæ›²ç©ºé—´æ± åŒ–")
            manifold = encoder.manifold

            # åœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡Œmasked pooling
            mask_expanded = mask.expand_as(z)

            # å°†æ— æ•ˆèŠ‚ç‚¹æŠ•å½±åˆ°åŸç‚¹ï¼ˆåœ¨åŒæ›²ç©ºé—´ä¸­ï¼‰
            z_masked = z * mask_expanded

            print(f"  åŒæ›²ç©ºé—´ - mask_expandedå½¢çŠ¶: {mask_expanded.shape}")
            print(f"  åŒæ›²ç©ºé—´ - z_maskedå½¢çŠ¶: {z_masked.shape}")

            # è®¡ç®—æœ‰æ•ˆèŠ‚ç‚¹æ•°
            num_valid_nodes = mask.sum(dim=1, keepdim=True).float()
            num_valid_nodes = torch.clamp(num_valid_nodes, min=1.0)

            print(f"  åŒæ›²ç©ºé—´ - num_valid_nodeså½¢çŠ¶: {num_valid_nodes.shape}")

            # åœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡Œå¹³å‡ï¼ˆä½¿ç”¨Einsteinä¸­ç‚¹ï¼‰
            # ç®€åŒ–ç‰ˆæœ¬ï¼šå…ˆè½¬æ¢åˆ°åˆ‡ç©ºé—´ï¼Œå¹³å‡ï¼Œå†æŠ•å½±å›æµå½¢
            z_tangent = manifold.logmap0(z_masked)

            print(f"  åŒæ›²ç©ºé—´ - z_tangentå½¢çŠ¶: {z_tangent.shape}")

            # åœ¨åˆ‡ç©ºé—´ä¸­å¹³å‡ - æ²¿èŠ‚ç‚¹ç»´åº¦(dim=1)æ±‚å’Œ
            graph_embeddings = z_tangent.sum(dim=1) / num_valid_nodes.squeeze(-1)

            print(f"  åŒæ›²ç©ºé—´ - åˆ‡ç©ºé—´å¹³å‡åå½¢çŠ¶: {graph_embeddings.shape}")

            # æŠ•å½±å›æµå½¢
            graph_embeddings = manifold.expmap0(graph_embeddings)

            print(f"  åŒæ›²ç©ºé—´ - æŠ•å½±å›æµå½¢åå½¢çŠ¶: {graph_embeddings.shape}")

        else:
            # æ¬§å‡ é‡Œå¾—ç©ºé—´ï¼šæ ‡å‡†å¹³å‡æ± åŒ–
            print(f"  ä½¿ç”¨æ¬§å‡ é‡Œå¾—ç©ºé—´æ± åŒ–")

            # mask: [batch_size, num_nodes, 1]
            # z: [batch_size, num_nodes, feature_dim]
            mask_expanded = mask.expand_as(z)  # [batch_size, num_nodes, feature_dim]
            z_masked = z * mask_expanded

            print(f"  mask_expandedå½¢çŠ¶: {mask_expanded.shape}")
            print(f"  z_maskedå½¢çŠ¶: {z_masked.shape}")

            # è®¡ç®—æœ‰æ•ˆèŠ‚ç‚¹æ•°ï¼Œæ²¿ç€èŠ‚ç‚¹ç»´åº¦æ±‚å’Œ
            num_valid_nodes = mask.sum(dim=1, keepdim=True).float()  # [batch_size, 1, 1]
            num_valid_nodes = torch.clamp(num_valid_nodes, min=1.0)

            print(f"  num_valid_nodeså½¢çŠ¶: {num_valid_nodes.shape}")

            # å¹³å‡æ± åŒ– - å¾—åˆ°å›¾çº§åˆ«åµŒå…¥ï¼Œæ²¿ç€èŠ‚ç‚¹ç»´åº¦(dim=1)æ±‚å’Œ
            graph_embeddings = z_masked.sum(dim=1) / num_valid_nodes.squeeze(
                -1
            )  # [batch_size, feature_dim]
            print(f"  æ± åŒ–å‰z_masked.sum(dim=1)å½¢çŠ¶: {z_masked.sum(dim=1).shape}")
            print(f"  é™¤æ³•å‰num_valid_nodes.squeeze(-1)å½¢çŠ¶: {num_valid_nodes.squeeze(-1).shape}")

        print(f"  å›¾çº§åµŒå…¥å½¢çŠ¶: {graph_embeddings.shape}")
        return graph_embeddings


def _augment_data(data, diffusion_model, k_augment=5):
    """ä½¿ç”¨åˆ†æ•°ç½‘ç»œè¿›è¡Œæ•°æ®å¢å¼ºï¼Œæ”¯æŒControlNetæ¡ä»¶ç”Ÿæˆ"""
    if diffusion_model is None:
        return data

    x = data["x"]  # [batch_size, num_nodes, num_features]
    adj = data["adj"]  # [batch_size, num_nodes, num_nodes]
    labels = data["labels"]  # [batch_size]

    batch_size = x.size(0)
    device = x.device

    try:
        # æ£€æŸ¥æ‰©æ•£æ¨¡å‹ç±»å‹
        model_type = diffusion_model.get("type", "traditional")

        if model_type == "controlnet":
            return _augment_with_controlnet(data, diffusion_model, k_augment)
        else:
            return _augment_traditional(data, diffusion_model, k_augment)

    except Exception as e:
        print(f"âš ï¸ æ•°æ®å¢å¼ºå¤±è´¥: {e}")
        return data


def _augment_with_controlnet(data, diffusion_model, k_augment=5):
    """ä½¿ç”¨ControlNetè¿›è¡ŒåŸºäºç±»åˆ«åŸå‹çš„æ•°æ®å¢å¼º"""
    x = data["x"]
    adj = data["adj"]
    labels = data["labels"]

    batch_size = x.size(0)
    device = x.device

    controlnet = diffusion_model["model"]
    manifold = diffusion_model["manifold"]

    # é¦–å…ˆä½¿ç”¨å½“å‰æ‰¹æ¬¡æ›´æ–°ç±»åˆ«åŸå‹
    graph_features = x.mean(dim=1)  # æå–å›¾çº§ç‰¹å¾ [batch, feat_dim]
    controlnet.update_prototypes_from_support(x, labels)

    augmented_x_list = [x]  # åŒ…å«åŸå§‹æ•°æ®
    augmented_adj_list = [adj]
    augmented_labels_list = [labels]

    print(f"ğŸ¯ åŸºäºç±»åˆ«åŸå‹çš„ControlNetæ•°æ®å¢å¼º: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆ{k_augment}ä¸ªå˜ä½“")

    with torch.no_grad():
        for aug_idx in range(k_augment):
            # 1. æ·»åŠ å™ªå£°åˆ°åŸå§‹å›¾
            noise_level = 0.1 + aug_idx * 0.05  # é€’å¢å™ªå£°æ°´å¹³
            noisy_x = x + torch.randn_like(x) * noise_level

            # 2. ä½¿ç”¨ControlNetç”ŸæˆåŸºäºç±»åˆ«åŸå‹çš„å¢å¼º
            # åˆ›å»ºéšæœºæ—¶é—´æ­¥
            t = torch.rand(batch_size, device=device) * 0.5 + 0.1  # [0.1, 0.6]

            # ç”Ÿæˆflagsï¼ˆå‡è®¾æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ•ˆï¼‰
            flags = torch.ones_like(x[..., 0], dtype=torch.bool, device=device)  # [batch, nodes]

            # ControlNetå‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨ç±»åˆ«åŸå‹æ¡ä»¶ï¼‰
            enhanced_x, _ = controlnet(
                x=noisy_x,
                adj=adj,
                flags=flags,
                t=t,
                graph_features=graph_features,  # å›¾çº§ç‰¹å¾
                class_labels=labels,  # ç±»åˆ«æ ‡ç­¾
            )

            # 3. åœ¨æµå½¢ä¸ŠæŠ•å½±ï¼ˆPoincarÃ©çƒçº¦æŸï¼‰
            enhanced_x = manifold.proj(enhanced_x)

            # 4. æ·»åŠ åˆ°å¢å¼ºåˆ—è¡¨
            augmented_x_list.append(enhanced_x)
            augmented_adj_list.append(adj)  # ä¿æŒæ‹“æ‰‘ç»“æ„
            augmented_labels_list.append(labels)

    # åˆå¹¶æ‰€æœ‰å¢å¼ºæ•°æ®
    final_x = torch.cat(augmented_x_list, dim=0)
    final_adj = torch.cat(augmented_adj_list, dim=0)
    final_labels = torch.cat(augmented_labels_list, dim=0)

    print(f"âœ“ åŸºäºç±»åˆ«åŸå‹çš„å¢å¼ºå®Œæˆ: {batch_size} â†’ {final_x.size(0)} æ ·æœ¬")
    print(f"   åŸå‹æ›´æ–°: å·²æ ¹æ®å½“å‰æ‰¹æ¬¡æ›´æ–°ç±»åˆ«åŸå‹")

    return {"x": final_x, "adj": final_adj, "labels": final_labels}


def _augment_traditional(data, diffusion_model, k_augment=5):
    """ä¼ ç»Ÿçš„åˆ†æ•°ç½‘ç»œæ•°æ®å¢å¼ºï¼ˆå…¼å®¹ä¹‹å‰çš„æ–¹æ³•ï¼‰"""
    x = data["x"]
    adj = data["adj"]
    labels = data["labels"]

    batch_size = x.size(0)
    device = x.device

    # è·å–åˆ†æ•°ç½‘ç»œç»„ä»¶
    score_x = diffusion_model["score_x"]
    manifold = diffusion_model["manifold"]

    augmented_x_list = [x]  # åŒ…å«åŸå§‹æ•°æ®
    augmented_adj_list = [adj]
    augmented_labels_list = [labels]

    print(f"ğŸ”§ ä¼ ç»Ÿæ•°æ®å¢å¼º: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆ{k_augment}ä¸ªå˜ä½“")

    with torch.no_grad():
        for aug_idx in range(k_augment):
            # æ·»åŠ å™ªå£°åˆ°åŸå§‹æ•°æ®
            noise_scale = 0.1 * (1 + aug_idx * 0.1)  # é€æ¸å¢åŠ å™ªå£°å¼ºåº¦

            # å¯¹èŠ‚ç‚¹ç‰¹å¾æ·»åŠ å™ªå£°
            noisy_x = x + torch.randn_like(x) * noise_scale

            # åœ¨æµå½¢ä¸ŠæŠ•å½±
            noisy_x = manifold.proj(noisy_x)

            # ä½¿ç”¨åˆ†æ•°ç½‘ç»œè¿›è¡Œå»å™ªï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ‰©æ•£è¿‡ç¨‹

            augmented_x_list.append(noisy_x)
            augmented_adj_list.append(adj)
            augmented_labels_list.append(labels)

    # åˆå¹¶æ•°æ®
    final_x = torch.cat(augmented_x_list, dim=0)
    final_adj = torch.cat(augmented_adj_list, dim=0)
    final_labels = torch.cat(augmented_labels_list, dim=0)

    print(f"âœ“ ä¼ ç»Ÿå¢å¼ºå®Œæˆ: {batch_size} â†’ {final_x.size(0)} æ ·æœ¬")

    return {"x": final_x, "adj": final_adj, "labels": final_labels}


def _train_classifier_on_task(
    task, encoder, diffusion_model, config, device, use_augmentation=False
):
    """åœ¨å•ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒåˆ†ç±»å™¨"""
    # è·å–æ•°æ®
    support_x = task["support_set"]["x"].to(device)
    support_adj = task["support_set"]["adj"].to(device)
    support_labels = task["support_set"]["label"].to(device)

    query_x = task["query_set"]["x"].to(device)
    query_adj = task["query_set"]["adj"].to(device)
    query_labels = task["query_set"]["label"].to(device)

    # æ•°æ®å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_augmentation and diffusion_model is not None:
        k_augment = getattr(config.fsl_task, "k_augment", 5)
        # å¢å¼ºæ”¯æŒé›†æ•°æ®
        support_data = {"x": support_x, "adj": support_adj, "labels": support_labels}
        augmented_support = _augment_data(support_data, diffusion_model, k_augment)

        # ä½¿ç”¨å¢å¼ºåçš„æ•°æ®
        support_x = augmented_support["x"]
        support_adj = augmented_support["adj"]
        support_labels = augmented_support["labels"]

    # è·å–åµŒå…¥
    support_emb = _get_embeddings(encoder, support_x, support_adj, device)
    query_emb = _get_embeddings(encoder, query_x, query_adj, device)

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„ - å…³é”®ä¿®å¤ï¼šä½¿ç”¨è¿ç»­çš„æ ‡ç­¾æ˜ å°„
    unique_support_labels = torch.unique(support_labels)
    unique_query_labels = torch.unique(query_labels)

    # ç¡®ä¿æŸ¥è¯¢æ ‡ç­¾éƒ½åœ¨æ”¯æŒæ ‡ç­¾ä¸­
    all_labels = torch.unique(torch.cat([support_labels, query_labels]))
    # åˆ›å»ºä»åŸå§‹æ ‡ç­¾åˆ°è¿ç»­æ ‡ç­¾çš„æ˜ å°„
    label_map = {label.item(): idx for idx, label in enumerate(all_labels)}

    # æ˜ å°„æ ‡ç­¾
    mapped_support_labels = torch.tensor(
        [label_map[label.item()] for label in support_labels], device=device, dtype=torch.long
    )
    mapped_query_labels = torch.tensor(
        [label_map[label.item()] for label in query_labels], device=device, dtype=torch.long
    )

    # åˆ›å»ºåˆ†ç±»å™¨
    num_classes = len(all_labels)
    # ä¿®å¤ï¼šsupport_embçš„å½¢çŠ¶å¯èƒ½æ˜¯[batch_size, num_nodes, embedding_dim]ï¼Œéœ€è¦è·å–æ­£ç¡®çš„ç»´åº¦
    if support_emb.dim() == 3:
        # å¦‚æœæ˜¯3ç»´ï¼Œè¯´æ˜æ˜¯[batch_size, num_nodes, embedding_dim]ï¼Œéœ€è¦æ± åŒ–
        embedding_dim = support_emb.size(-1)

        # æŒ‰ç…§GraphVAEä¸­çš„å¤„ç†æ–¹å¼ï¼šmean + max poolingç„¶åè¿æ¥
        support_mean = support_emb.mean(dim=1)  # [batch_size, embedding_dim]
        support_max = support_emb.max(dim=1).values  # [batch_size, embedding_dim]
        support_emb_concat = torch.cat(
            [support_mean, support_max], dim=-1
        )  # [batch_size, embedding_dim*2]

        query_mean = query_emb.mean(dim=1)
        query_max = query_emb.max(dim=1).values
        query_emb_concat = torch.cat([query_mean, query_max], dim=-1)

        model_dim = embedding_dim  # ClassifieræœŸæœ›çš„model_dimæ˜¯å•ä¸ªåµŒå…¥ç»´åº¦
    else:
        # å¦‚æœæ˜¯2ç»´ï¼Œè¯´æ˜å·²ç»æ˜¯[batch_size, embedding_dim]
        # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬å‡è®¾å·²ç»æ˜¯å¤„ç†è¿‡çš„ç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨
        model_dim = support_emb.size(1) // 2  # å‡è®¾å·²ç»æ˜¯è¿æ¥åçš„ç‰¹å¾
        support_emb_concat = support_emb
        query_emb_concat = query_emb

    classifier = Classifier(
        model_dim=model_dim,
        num_classes=num_classes,
        classifier_dropout=0.2,
        classifier_bias=True,
        manifold=None,  # ä½¿ç”¨æ¬§å‡ é‡Œå¾—ç©ºé—´
    ).to(device)

    # è®­ç»ƒåˆ†ç±»å™¨ - æ”¹è¿›è®­ç»ƒè¿‡ç¨‹
    optimizer = optim.Adam(classifier.parameters(), lr=0.01, weight_decay=0.0001)
    criterion = nn.CrossEntropyLoss()

    classifier.train()

    # å¢åŠ è®­ç»ƒè½®æ•°å¹¶æ·»åŠ éªŒè¯
    num_epochs = 100
    best_val_loss = float("inf")
    patience = 10
    no_improve_count = 0

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        logits = classifier(support_emb_concat)
        loss = criterion(logits, mapped_support_labels)
        loss.backward()
        optimizer.step()

        # ç®€å•çš„éªŒè¯ - åœ¨æ”¯æŒé›†ä¸Šæµ‹è¯•
        if epoch % 10 == 0:
            classifier.eval()
            with torch.no_grad():
                val_logits = classifier(support_emb_concat)
                val_loss = criterion(val_logits, mapped_support_labels)
                val_preds = torch.argmax(val_logits, dim=1)
                val_acc = (val_preds == mapped_support_labels).float().mean()

                print(
                    f"  Epoch {epoch}: loss={loss.item():.4f}, val_loss={val_loss.item():.4f}, val_acc={val_acc.item():.4f}"
                )

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    no_improve_count = 0
                else:
                    no_improve_count += 1

                if no_improve_count >= patience:
                    print(f"  Early stopping at epoch {epoch}")
                    break

            classifier.train()

    # è¯„ä¼°
    classifier.eval()
    with torch.no_grad():
        query_logits = classifier(query_emb_concat)
        query_preds = torch.argmax(query_logits, dim=1)

        # è®¡ç®—æŒ‡æ ‡
        accuracy = (query_preds == mapped_query_labels).float().mean().item()

        y_true = mapped_query_labels.cpu().numpy()
        y_pred = query_preds.cpu().numpy()

        f1 = f1_score(y_true, y_pred, average="macro", zero_division=0)
        precision = precision_score(y_true, y_pred, average="macro", zero_division=0)
        recall = recall_score(y_true, y_pred, average="macro", zero_division=0)

    return {
        "accuracy": accuracy,
        "f1": f1,
        "precision": precision,
        "recall": recall,
    }


def _run_evaluation(dataset, encoder, diffusion_model, config, device, use_augmentation=False):
    """è¿è¡Œå…ƒæµ‹è¯•è¯„ä¼° - ä½¿ç”¨æ–°çš„ä¸‰é˜¶æ®µControlNetå¾®è°ƒæµç¨‹"""
    print(f"ğŸš€ Starting meta-test... (å¢å¼ºæ¨¡å¼: {use_augmentation})")

    results = []
    num_tasks = getattr(config.fsl_task, "num_test_tasks", 100)

    progress_bar = tqdm(range(num_tasks), desc="Meta-test")

    # å‡†å¤‡æ¨¡å‹ç»„ä»¶
    model_components = {"encoder": encoder, "diffusion_model": diffusion_model}

    for task_idx in progress_bar:
        try:
            # é‡‡æ ·ä»»åŠ¡ - ä¿®å¤å‚æ•°
            task = dataset.sample_one_task(
                is_train=False,  # ä»æµ‹è¯•é›†é‡‡æ ·
                N_way=config.fsl_task.N_way,
                K_shot=config.fsl_task.K_shot,
                R_query=config.fsl_task.R_query,
            )

            if task is None:
                continue

            # è½¬æ¢ä»»åŠ¡æ ¼å¼ä»¥é€‚é…æ–°çš„meta_test_single_taskå‡½æ•°
            task_data = {
                "support": {
                    "x": task["support_set"]["x"],
                    "adj": task["support_set"]["adj"],
                    "labels": task["support_set"]["label"],
                },
                "query": {
                    "x": task["query_set"]["x"],
                    "adj": task["query_set"]["adj"],
                    "labels": task["query_set"]["label"],
                },
            }

            # ä½¿ç”¨æ–°çš„ä¸‰é˜¶æ®µæµç¨‹ï¼šControlNetå¾®è°ƒ â†’ æ•°æ®å¢å¼º â†’ åˆ†ç±»å™¨è®­ç»ƒå’Œè¯„ä¼°
            result = meta_test_single_task(
                task_data=task_data, model_components=model_components, config=config, device=device
            )

            # è½¬æ¢ç»“æœæ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            if "accuracy" in result:
                compatible_result = {
                    "accuracy": result["accuracy"],
                    "f1": result.get("accuracy", 0.0),  # ä½¿ç”¨accuracyä½œä¸ºf1çš„fallback
                }
                results.append(compatible_result)

            # æ›´æ–°è¿›åº¦æ¡
            if results:
                avg_acc = np.mean([r["accuracy"] for r in results])
                progress_bar.set_postfix({"Avg Acc": f"{avg_acc:.4f}"})

            # è®°å½•ä¸­é—´ç»“æœ
            if (task_idx + 1) % 10 == 0:
                avg_acc = np.mean([r["accuracy"] for r in results])
                avg_f1 = np.mean([r["f1"] for r in results])
                wandb.log(
                    {
                        "avg_accuracy": avg_acc,
                        "avg_f1": avg_f1,
                        "completed_tasks": task_idx + 1,
                    }
                )

        except Exception as e:
            print(f"âš ï¸ ä»»åŠ¡ {task_idx} å¤±è´¥: {e}")
            continue

    # è®¡ç®—æœ€ç»ˆç»“æœ
    if results:
        accuracies = [r["accuracy"] for r in results]
        f1_scores = [r["f1"] for r in results]

        final_acc = np.mean(accuracies)
        final_f1 = np.mean(f1_scores)
        std_acc = np.std(accuracies)
        std_f1 = np.std(f1_scores)

        # 95%ç½®ä¿¡åŒºé—´
        margin_acc = 1.96 * std_acc / np.sqrt(len(accuracies))
        margin_f1 = 1.96 * std_f1 / np.sqrt(len(f1_scores))

        # è®°å½•æœ€ç»ˆç»“æœ
        wandb.log(
            {
                "final_accuracy": final_acc,
                "final_f1": final_f1,
                "accuracy_std": std_acc,
                "f1_std": std_f1,
                "accuracy_margin": margin_acc,
                "f1_margin": margin_f1,
                "num_tasks": len(results),
                "use_augmentation": use_augmentation,
            }
        )

        # æ‰“å°ç»“æœ
        aug_status = "ControlNetå¾®è°ƒå¢å¼ºæ¨¡å¼" if use_augmentation else "åŸºç¡€æ¨¡å¼"
        print("\n" + "=" * 60)
        print(f"ğŸ“Š FINAL RESULTS ({aug_status})")
        print("=" * 60)
        print(f"Number of tasks: {len(results)}")
        print(f"Accuracy: {final_acc:.4f} Â± {margin_acc:.4f}")
        print(f"F1 Score: {final_f1:.4f} Â± {margin_f1:.4f}")
        print("=" * 60)

        return {
            "accuracy": final_acc,
            "f1": final_f1,
            "num_tasks": len(results),
            "use_augmentation": use_augmentation,
        }
    else:
        print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•ä»»åŠ¡!")
        return {
            "accuracy": 0.0,
            "f1": 0.0,
            "num_tasks": 0,
            "use_augmentation": use_augmentation,
        }


def existing_task_test(encoder, graph_embedding_net, task, config, device, use_augmentation=False):
    """
    å¯¹å•ä¸ªä»»åŠ¡è¿›è¡Œæµ‹è¯•ï¼Œä½¿ç”¨ç°æœ‰çš„æ–¹æ³•
    """
    try:
        # æå–ä»»åŠ¡æ•°æ®
        support_x = task["support_set"]["x"].to(device)
        support_adj = task["support_set"]["adj"].to(device)
        support_labels = task["support_set"]["y"].to(device)
        query_x = task["query_set"]["x"].to(device)
        query_adj = task["query_set"]["adj"].to(device)
        query_labels = task["query_set"]["y"].to(device)

        N_way = config.fsl_task.N_way

        with torch.no_grad():
            # ç¼–ç å›¾å½¢
            support_node_emb = encoder(support_x, support_adj)
            query_node_emb = encoder(query_x, query_adj)

            # è·å–å›¾çº§åˆ«åµŒå…¥
            support_graph_emb = graph_embedding_net(support_node_emb, support_adj)
            query_graph_emb = graph_embedding_net(query_node_emb, query_adj)

            support_emb = support_graph_emb
            query_emb = query_graph_emb

            # æ ‡ç­¾ä¿¡æ¯
            unique_labels = torch.unique(torch.cat([support_labels, query_labels]))

            # æ ‡ç­¾æ˜ å°„ï¼šå°†åŸå§‹æ ‡ç­¾æ˜ å°„åˆ°0, 1, 2, ..., N_way-1
            label_to_new = {label.item(): i for i, label in enumerate(unique_labels)}

            mapped_support_labels = torch.tensor(
                [label_to_new[label.item()] for label in support_labels], device=device
            )
            mapped_query_labels = torch.tensor(
                [label_to_new[label.item()] for label in query_labels], device=device
            )

            # è¿æ¥å›¾åµŒå…¥å’ŒèŠ‚ç‚¹åµŒå…¥ç‰¹å¾
            support_emb_concat = torch.cat([support_emb, support_node_emb.mean(dim=1)], dim=-1)
            query_emb_concat = torch.cat([query_emb, query_node_emb.mean(dim=1)], dim=-1)

            # è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨
            classifier = nn.Linear(support_emb_concat.size(-1), N_way).to(device)
            optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01)
            criterion = nn.CrossEntropyLoss()

            # è®­ç»ƒ100ä¸ªepoch
            for epoch in range(100):
                optimizer.zero_grad()
                logits = classifier(support_emb_concat)
                loss = criterion(logits, mapped_support_labels)
                loss.backward()
                optimizer.step()

            # æµ‹è¯•
            with torch.no_grad():
                query_logits = classifier(query_emb_concat)
                query_preds = torch.argmax(query_logits, dim=1)

                # è®¡ç®—å‡†ç¡®ç‡
                correct = (query_preds == mapped_query_labels).sum().item()
                total = mapped_query_labels.size(0)
                accuracy = correct / total

                return {"accuracy": accuracy}

    except Exception as e:
        print(f"Task test error: {e}")
        return {"accuracy": 0.0}


def _finetune_controlnet_for_task(controlnet, support_data, config, device):
    """
    ä¸ºç‰¹å®šä»»åŠ¡å¾®è°ƒControlNet

    Args:
        controlnet: ControlNetæ¨¡å‹
        support_data: æ”¯æŒé›†æ•°æ®
        config: é…ç½®
        device: è®¾å¤‡
    """
    print(f"ğŸ”§ å¼€å§‹ä»»åŠ¡ç‰¹å®šçš„ControlNetå¾®è°ƒ...")

    # æå–æ”¯æŒé›†æ•°æ®å¹¶ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    support_x = support_data["x"].to(device)  # [num_support, nodes, features]
    support_adj = support_data["adj"].to(device)
    support_labels = support_data["labels"].to(device)

    # 1. é¦–å…ˆæ›´æ–°ç±»åˆ«åŸå‹
    controlnet.update_prototypes_from_support(support_x, support_labels)
    print(f"âœ“ ç±»åˆ«åŸå‹å·²æ›´æ–°")

    # 2. è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–ControlNetåˆ†æ”¯ï¼‰
    controlnet_params = [p for p in controlnet.parameters() if p.requires_grad]
    optimizer = torch.optim.Adam(controlnet_params, lr=config.get("controlnet_lr", 0.001))

    # 3. å¾®è°ƒå‚æ•°
    num_finetune_epochs = config.get("controlnet_finetune_epochs", 10)
    noise_scale = config.get("finetune_noise_scale", 0.1)

    controlnet.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

    finetune_losses = []

    for epoch in range(num_finetune_epochs):
        total_loss = 0.0
        num_batches = 0

        # å¯¹æ”¯æŒé›†è¿›è¡Œå¤šæ¬¡å¾®è°ƒ
        batch_size = min(4, len(support_x))  # å°æ‰¹é‡

        for start_idx in range(0, len(support_x), batch_size):
            end_idx = min(start_idx + batch_size, len(support_x))

            # è·å–å½“å‰æ‰¹æ¬¡
            batch_x = support_x[start_idx:end_idx]
            batch_adj = support_adj[start_idx:end_idx]
            batch_labels = support_labels[start_idx:end_idx]

            # æå–å›¾çº§ç‰¹å¾
            graph_features = batch_x.mean(dim=1)

            # æ·»åŠ å™ªå£°åˆ›å»ºè®­ç»ƒå¯¹
            noisy_x = batch_x + torch.randn_like(batch_x) * noise_scale

            # åˆ›å»ºæ—¶é—´æ­¥å’Œflags
            t = torch.rand(batch_x.size(0), device=device) * 0.5 + 0.1
            flags = torch.ones_like(batch_x[..., 0], dtype=torch.bool, device=device)

            # ControlNetå‰å‘ä¼ æ’­
            predicted_x, _ = controlnet(
                x=noisy_x,
                adj=batch_adj,
                flags=flags,
                t=t,
                graph_features=graph_features,
                class_labels=batch_labels,
            )

            # è®¡ç®—é‡å»ºæŸå¤± - ç›®æ ‡æ˜¯ä»å™ªå£°å›¾é‡å»ºåŸå§‹å›¾
            reconstruction_loss = torch.nn.functional.mse_loss(predicted_x, batch_x)

            # æ·»åŠ æ­£åˆ™åŒ–ï¼šä¿æŒç±»åˆ«ç‰¹å¾ä¸€è‡´æ€§
            # è®¡ç®—ç”Ÿæˆæ ·æœ¬çš„ç±»åˆ«ç‰¹å¾ä¸åŸå§‹æ ·æœ¬çš„ç›¸ä¼¼æ€§
            generated_features = predicted_x.mean(dim=1)  # å›¾çº§ç‰¹å¾
            original_features = batch_x.mean(dim=1)
            consistency_loss = torch.nn.functional.mse_loss(generated_features, original_features)

            # æ€»æŸå¤±
            loss = reconstruction_loss + 0.1 * consistency_loss

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(controlnet_params, max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        finetune_losses.append(avg_loss)

        if epoch % 2 == 0 or epoch == num_finetune_epochs - 1:
            print(f"  å¾®è°ƒ Epoch {epoch:2d}: loss={avg_loss:.4f}")

    controlnet.eval()  # è®¾ç½®å›è¯„ä¼°æ¨¡å¼

    print(f"âœ“ ControlNetå¾®è°ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {finetune_losses[-1]:.4f}")
    return controlnet


def _augment_with_finetuned_controlnet(data, controlnet, config, k_augment=5):
    """
    ä½¿ç”¨å¾®è°ƒåçš„ControlNetè¿›è¡Œæ•°æ®å¢å¼º

    Args:
        data: è¾“å…¥æ•°æ®ï¼ˆæ”¯æŒé›†æˆ–æŸ¥è¯¢é›†ï¼‰
        controlnet: å¾®è°ƒåçš„ControlNet
        config: é…ç½®
        k_augment: å¢å¼ºå€æ•°
    """
    x = data["x"]
    adj = data["adj"]
    labels = data["labels"]

    batch_size = x.size(0)
    device = x.device

    # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    x = x.to(device)
    adj = adj.to(device)
    labels = labels.to(device)

    augmented_x_list = [x]  # åŒ…å«åŸå§‹æ•°æ®
    augmented_adj_list = [adj]
    augmented_labels_list = [labels]

    print(f"ğŸ¯ ä½¿ç”¨å¾®è°ƒåçš„ControlNetè¿›è¡Œæ•°æ®å¢å¼º: {k_augment}x")

    with torch.no_grad():
        # æå–å›¾çº§ç‰¹å¾
        graph_features = x.mean(dim=1)

        for aug_idx in range(k_augment):
            # ä½¿ç”¨ä¸åŒçš„å™ªå£°æ°´å¹³å’Œæ—¶é—´æ­¥
            noise_level = 0.05 + aug_idx * 0.03  # æ›´ç»†è‡´çš„å™ªå£°æ§åˆ¶
            time_step = 0.1 + aug_idx * 0.1  # ä¸åŒçš„æ—¶é—´æ­¥

            # æ·»åŠ å™ªå£°
            noisy_x = x + torch.randn_like(x) * noise_level

            # åˆ›å»ºæ—¶é—´æ­¥å’Œflags
            t = torch.full((batch_size,), time_step, device=device)
            flags = torch.ones_like(x[..., 0], dtype=torch.bool, device=device)

            # ControlNetç”Ÿæˆ
            enhanced_x, _ = controlnet(
                x=noisy_x,
                adj=adj,
                flags=flags,
                t=t,
                graph_features=graph_features,
                class_labels=labels,
            )

            # ç¡®ä¿ç”Ÿæˆçš„æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            enhanced_x = enhanced_x.to(device)

            # å¯é€‰ï¼šåœ¨æµå½¢ä¸ŠæŠ•å½±
            # enhanced_x = manifold.proj(enhanced_x)  # å¦‚æœéœ€è¦çš„è¯

            augmented_x_list.append(enhanced_x)
            augmented_adj_list.append(adj)  # ä¿æŒæ‹“æ‰‘ç»“æ„
            augmented_labels_list.append(labels)

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    final_x = torch.cat(augmented_x_list, dim=0)
    final_adj = torch.cat(augmented_adj_list, dim=0)
    final_labels = torch.cat(augmented_labels_list, dim=0)

    print(f"âœ“ å¾®è°ƒåå¢å¼ºå®Œæˆ: {batch_size} â†’ {final_x.size(0)} æ ·æœ¬")

    return {"x": final_x, "adj": final_adj, "labels": final_labels}


def meta_test_single_task(task_data, model_components, config, device):
    """
    æ‰§è¡Œå•ä¸ªä»»åŠ¡çš„meta-testï¼ŒåŒ…å«ä¸‰é˜¶æ®µæµç¨‹ï¼š
    1. ControlNetå¾®è°ƒ
    2. æ•°æ®å¢å¼º
    3. åˆ†ç±»å™¨è®­ç»ƒå’Œè¯„ä¼°
    """
    try:
        support_data = task_data["support"]
        query_data = task_data["query"]

        # ==================== é˜¶æ®µ1: ControlNetä»»åŠ¡ç‰¹å®šå¾®è°ƒ ====================
        diffusion_model = model_components.get("diffusion_model")
        use_finetuned_augmentation = config.get("use_finetuned_controlnet", True)

        if (
            diffusion_model is not None
            and diffusion_model.get("type") == "controlnet"
            and use_finetuned_augmentation
        ):
            print(f"ğŸ”§ é˜¶æ®µ1: ControlNetä»»åŠ¡ç‰¹å®šå¾®è°ƒ")

            # åˆ›å»ºControlNetçš„å‰¯æœ¬è¿›è¡Œå¾®è°ƒï¼ˆé¿å…å½±å“å…¶ä»–ä»»åŠ¡ï¼‰
            import copy

            task_controlnet = copy.deepcopy(diffusion_model["model"])

            # å¾®è°ƒControlNet
            task_controlnet = _finetune_controlnet_for_task(
                controlnet=task_controlnet, support_data=support_data, config=config, device=device
            )

            # ==================== é˜¶æ®µ2: æ•°æ®å¢å¼º ====================
            print(f"ğŸ¯ é˜¶æ®µ2: ä½¿ç”¨å¾®è°ƒåçš„ControlNetè¿›è¡Œæ•°æ®å¢å¼º")

            # å¢å¼ºæ”¯æŒé›†
            k_augment_support = config.get("k_augment_support", 3)
            augmented_support = _augment_with_finetuned_controlnet(
                data=support_data,
                controlnet=task_controlnet,
                config=config,
                k_augment=k_augment_support,
            )

            # å¯é€‰ï¼šä¹Ÿå¢å¼ºæŸ¥è¯¢é›†ï¼ˆç”¨äºè®­ç»ƒåˆ†ç±»å™¨ï¼Œä½†ä¸ç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
            if config.get("augment_query_for_training", False):
                k_augment_query = config.get("k_augment_query", 2)
                augmented_query_for_training = _augment_with_finetuned_controlnet(
                    data=query_data,
                    controlnet=task_controlnet,
                    config=config,
                    k_augment=k_augment_query,
                )
            else:
                augmented_query_for_training = None

        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¼ ç»Ÿåˆ†æ•°ç½‘ç»œå¯ç”¨äºå¢å¼º
            if diffusion_model is not None and diffusion_model.get("type") == "traditional":
                print(f"ğŸ”§ é˜¶æ®µ2: ä½¿ç”¨ä¼ ç»ŸScoreç½‘ç»œè¿›è¡Œæ•°æ®å¢å¼º")

                # ä½¿ç”¨ä¼ ç»Ÿå¢å¼ºæ–¹æ³•
                k_augment_support = config.get("k_augment_support", 3)
                augmented_support = _augment_traditional(
                    data=support_data,
                    diffusion_model=diffusion_model,
                    k_augment=k_augment_support,
                )

                # å¯é€‰ï¼šä¹Ÿå¢å¼ºæŸ¥è¯¢é›†
                if config.get("augment_query_for_training", False):
                    k_augment_query = config.get("k_augment_query", 2)
                    augmented_query_for_training = _augment_traditional(
                        data=query_data,
                        diffusion_model=diffusion_model,
                        k_augment=k_augment_query,
                    )
                else:
                    augmented_query_for_training = None
            else:
                print(f"âš ï¸ è·³è¿‡ControlNetå¾®è°ƒï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                augmented_support = support_data
                augmented_query_for_training = None

        # ==================== é˜¶æ®µ3: åˆ†ç±»å™¨è®­ç»ƒå’Œè¯„ä¼° ====================
        print(f"ğŸ“Š é˜¶æ®µ3: åˆ†ç±»å™¨è®­ç»ƒå’Œè¯„ä¼°")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        if augmented_query_for_training is not None:
            # åˆå¹¶å¢å¼ºçš„æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†ç”¨äºè®­ç»ƒ
            train_x = torch.cat([augmented_support["x"], augmented_query_for_training["x"]], dim=0)
            train_adj = torch.cat(
                [augmented_support["adj"], augmented_query_for_training["adj"]], dim=0
            )
            train_labels = torch.cat(
                [augmented_support["labels"], augmented_query_for_training["labels"]], dim=0
            )
            train_data = {"x": train_x, "adj": train_adj, "labels": train_labels}
        else:
            # åªä½¿ç”¨å¢å¼ºçš„æ”¯æŒé›†
            train_data = augmented_support

        # è®­ç»ƒåˆ†ç±»å™¨
        classifier = _train_classifier(
            train_data=train_data, model_components=model_components, config=config, device=device
        )

        # åœ¨åŸå§‹æŸ¥è¯¢é›†ä¸Šè¯„ä¼°ï¼ˆé‡è¦ï¼šè¯„ä¼°æ—¶ä¸ä½¿ç”¨å¢å¼ºæ•°æ®ï¼‰
        test_results = _evaluate_classifier(
            classifier=classifier,
            test_data=query_data,  # ä½¿ç”¨åŸå§‹æŸ¥è¯¢é›†
            config=config,
            device=device,
        )

        # è®°å½•å„é˜¶æ®µä¿¡æ¯
        results = {
            "accuracy": test_results["accuracy"],
            "loss": test_results.get("loss", 0.0),
            "stages": {
                "controlnet_finetuned": diffusion_model is not None and use_finetuned_augmentation,
                "support_augmented": augmented_support["x"].size(0),
                "original_support": support_data["x"].size(0),
                "augmentation_ratio": augmented_support["x"].size(0) / support_data["x"].size(0),
            },
        }

        print(
            f"âœ“ ä»»åŠ¡å®Œæˆ - å‡†ç¡®ç‡: {results['accuracy']:.4f}, "
            f"å¢å¼ºæ¯”ä¾‹: {results['stages']['augmentation_ratio']:.1f}x"
        )

        return results

    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return {
            "accuracy": 0.0,
            "error": str(e),
            "stages": {
                "controlnet_finetuned": False,
                "support_augmented": 0,
                "original_support": 0,
                "augmentation_ratio": 0.0,
            },
        }


def _train_classifier(train_data, model_components, config, device):
    """è®­ç»ƒåˆ†ç±»å™¨"""
    encoder = model_components["encoder"]

    x = train_data["x"].to(device)
    adj = train_data["adj"].to(device)
    labels = train_data["labels"].to(device)

    # è·å–åµŒå…¥
    with torch.no_grad():
        embeddings = _get_embeddings(encoder, x, adj, device)

    print(f"  åµŒå…¥å½¢çŠ¶: {embeddings.shape}, æ ‡ç­¾å½¢çŠ¶: {labels.shape}")

    # æ ‡ç­¾é‡æ–°æ˜ å°„ï¼šå°†åŸå§‹æ ‡ç­¾æ˜ å°„åˆ°è¿ç»­çš„0, 1, 2, ..., N_way-1
    unique_labels = torch.unique(labels)
    label_to_new = {label.item(): i for i, label in enumerate(unique_labels)}

    mapped_labels = torch.tensor(
        [label_to_new[label.item()] for label in labels], device=device, dtype=torch.long
    )

    print(f"  æ ‡ç­¾æ˜ å°„: {dict(zip(unique_labels.tolist(), range(len(unique_labels))))}")
    print(f"  æ˜ å°„åæ ‡ç­¾å½¢çŠ¶: {mapped_labels.shape}, ç±»åˆ«æ•°: {len(unique_labels)}")

    # åˆ›å»ºåˆ†ç±»å™¨ - ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥ç»´åº¦
    input_dim = embeddings.size(-1)  # æœ€åä¸€ä¸ªç»´åº¦æ˜¯ç‰¹å¾ç»´åº¦
    num_classes = len(unique_labels)  # ä½¿ç”¨å®é™…çš„ç±»åˆ«æ•°

    classifier_config = getattr(config, "meta_test", {}).get("classifier", {})

    classifier = nn.Sequential(
        nn.Linear(input_dim, 64),
        nn.ReLU(),
        nn.Dropout(classifier_config.get("dropout", 0.1)),
        nn.Linear(64, num_classes),
    ).to(device)

    # è®­ç»ƒåˆ†ç±»å™¨
    optimizer = torch.optim.Adam(
        classifier.parameters(),
        lr=classifier_config.get("lr", 0.001),
        weight_decay=classifier_config.get("weight_decay", 0.0001),
    )

    criterion = nn.CrossEntropyLoss()
    epochs = classifier_config.get("epochs", 100)

    classifier.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        logits = classifier(embeddings)
        loss = criterion(logits, mapped_labels)  # ä½¿ç”¨æ˜ å°„åçš„æ ‡ç­¾
        loss.backward()
        optimizer.step()

        # æ¯10ä¸ªepochæ‰“å°ä¸€æ¬¡
        if epoch % 10 == 0:
            val_acc = torch.mean((torch.argmax(logits, dim=1) == mapped_labels).float())
            print(
                f"  Epoch {epoch}: loss={loss.item():.4f}, val_loss={loss.item():.4f}, val_acc={val_acc:.4f}"
            )

    classifier.eval()
    # è¿”å›åˆ†ç±»å™¨ã€ç¼–ç å™¨å’Œæ ‡ç­¾æ˜ å°„
    return (classifier, encoder, label_to_new)


def _evaluate_classifier(classifier, test_data, config, device):
    """è¯„ä¼°åˆ†ç±»å™¨"""
    classifier_model, encoder, label_to_new = classifier  # è§£åŒ…åˆ†ç±»å™¨ã€ç¼–ç å™¨å’Œæ ‡ç­¾æ˜ å°„

    x = test_data["x"].to(device)
    adj = test_data["adj"].to(device)
    labels = test_data["labels"].to(device)

    # å°†æµ‹è¯•æ ‡ç­¾æ˜ å°„åˆ°è®­ç»ƒæ—¶çš„æ ‡ç­¾ç©ºé—´
    mapped_test_labels = torch.tensor(
        [label_to_new[label.item()] for label in labels], device=device, dtype=torch.long
    )

    with torch.no_grad():
        # è·å–åµŒå…¥
        embeddings = _get_embeddings(encoder, x, adj, device)

        # åˆ†ç±»é¢„æµ‹
        logits = classifier_model(embeddings)
        predictions = torch.argmax(logits, dim=1)

        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆåœ¨æ˜ å°„çš„æ ‡ç­¾ç©ºé—´ä¸­ï¼‰
        correct = (predictions == mapped_test_labels).sum().item()
        total = mapped_test_labels.size(0)
        accuracy = correct / total

        # è®¡ç®—æŸå¤±
        criterion = nn.CrossEntropyLoss()
        loss = criterion(logits, mapped_test_labels)

        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")

    return {"accuracy": accuracy, "loss": loss.item()}

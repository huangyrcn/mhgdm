"""
Meta-testè¯„ä¼°å‡½æ•°
æ”¯æŒä½¿ç”¨encoderæˆ–encoder+åˆ†æ•°ç½‘ç»œè¿›è¡Œfew-shotå­¦ä¹ è¯„ä¼°
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import wandb
from tqdm import tqdm
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from omegaconf import OmegaConf

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.config_utils import load_config
from utils.data_utils import MyDataset
from utils.loader import load_seed, load_device
from utils.graph_utils import node_flags
from models.GraphVAE import GraphVAE
from models.Decoders import Classifier


def run_meta_test(config, use_augmentation=False, checkpoint_paths=None):
    """
    è¿è¡ŒMeta-testè¯„ä¼°

    Args:
        config: é…ç½®å¯¹è±¡
        use_augmentation: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼ºï¼ˆåˆ†æ•°ç½‘ç»œï¼‰
        checkpoint_paths: æ£€æŸ¥ç‚¹è·¯å¾„å­—å…¸ï¼ŒåŒ…å«:
            - vae_checkpoint: VAEæ£€æŸ¥ç‚¹è·¯å¾„
            - score_checkpoint: Scoreæ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆä»…åœ¨use_augmentation=Trueæ—¶éœ€è¦ï¼‰

    Returns:
        dict: è¯„ä¼°ç»“æœï¼ŒåŒ…å«accuracy, f1ç­‰æŒ‡æ ‡
    """
    if checkpoint_paths is None or "vae_checkpoint" not in checkpoint_paths:
        raise ValueError("å¿…é¡»æä¾›VAEæ£€æŸ¥ç‚¹è·¯å¾„")

    vae_checkpoint_path = checkpoint_paths["vae_checkpoint"]

    # ç¡®ä¿wandbä¼šè¯å¹²å‡€
    try:
        if wandb.run is not None:
            wandb.finish()
            print("âœ“ å·²å…³é—­ä¹‹å‰çš„wandbä¼šè¯")
    except:
        pass

    # åŸºç¡€è®¾ç½®
    device = load_device(config)
    load_seed(config.seed)

    # åˆå§‹åŒ–wandb
    wandb_suffix = "_aug" if use_augmentation else "_no_aug"
    mode = (
        "disabled"
        if getattr(config, "debug", False)
        else ("online" if config.wandb.online else "offline")
    )

    wandb.init(
        project=f"{config.wandb.project}_Meta",
        entity=config.wandb.entity,
        name=f"{config.run_name}_meta{wandb_suffix}",
        config=OmegaConf.to_container(config, resolve=True),
        mode=mode,
    )

    # åŠ è½½æ•°æ®é›†
    dataset = MyDataset(config.data, config.fsl_task)
    train_loader, test_loader = dataset.get_loaders()

    # åŠ è½½ç¼–ç å™¨
    encoder = _load_encoder(vae_checkpoint_path, device)

    # å¦‚æœä½¿ç”¨å¢å¼ºï¼Œè¿˜éœ€è¦åŠ è½½åˆ†æ•°ç½‘ç»œ
    diffusion_model = None
    if use_augmentation and "score_checkpoint" in checkpoint_paths:
        score_checkpoint_path = checkpoint_paths["score_checkpoint"]
        try:
            diffusion_model = _load_diffusion_model(score_checkpoint_path, config, device)
            print(f"âœ“ åˆ†æ•°ç½‘ç»œå·²åŠ è½½ï¼Œå¯ç”¨æ•°æ®å¢å¼º")
        except Exception as e:
            print(f"âš ï¸ åˆ†æ•°ç½‘ç»œåŠ è½½å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨å¢å¼º: {e}")
            diffusion_model = None

    print(f"Meta-test åˆå§‹åŒ–å®Œæˆ")
    print(f"Device: {device}")
    print(f"æ•°æ®å¢å¼º: {'å¯ç”¨' if diffusion_model is not None else 'ç¦ç”¨'}")

    # è¿è¡Œè¯„ä¼°
    results = _run_evaluation(
        dataset=dataset,
        encoder=encoder,
        diffusion_model=diffusion_model,
        config=config,
        device=device,
        use_augmentation=use_augmentation and diffusion_model is not None,
    )

    wandb.finish()
    return results


def _load_encoder(checkpoint_path, device):
    """åŠ è½½ç¼–ç å™¨"""
    print(f"Loading encoder from: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    vae_config = checkpoint["model_config"]

    # æ„å»ºVAEé…ç½®
    from types import SimpleNamespace

    model_config = SimpleNamespace()

    if "vae" in vae_config:
        # æ—§æ ¼å¼
        model_config.encoder_config = SimpleNamespace(**vae_config["vae"]["encoder"])
        model_config.decoder_config = SimpleNamespace(**vae_config["vae"]["decoder"])
        model_config.pred_node_class = vae_config["vae"]["loss"]["pred_node_class"]
        model_config.pred_edge = vae_config["vae"]["loss"]["pred_edge"]
        model_config.pred_graph_class = vae_config["vae"]["loss"]["pred_graph_class"]
        model_config.use_kl_loss = vae_config["vae"]["loss"]["use_kl_loss"]
        model_config.latent_dim = vae_config["vae"]["encoder"]["latent_feature_dim"]
    else:
        # æ–°æ ¼å¼
        model_config.encoder_config = SimpleNamespace(**vae_config["encoder"])
        model_config.decoder_config = SimpleNamespace(**vae_config["decoder"])
        model_config.pred_node_class = vae_config["loss"]["pred_node_class"]
        model_config.pred_edge = vae_config["loss"]["pred_edge"]
        model_config.pred_graph_class = vae_config["loss"]["pred_graph_class"]
        model_config.use_kl_loss = vae_config["loss"]["use_kl_loss"]
        model_config.latent_dim = vae_config["encoder"]["latent_feature_dim"]

    model_config.use_base_proto_loss = False
    model_config.use_sep_proto_loss = False
    model_config.device = device

    # åˆ›å»ºå¹¶åŠ è½½VAE
    vae_model = GraphVAE(model_config)
    vae_model.load_state_dict(checkpoint["model_state_dict"])
    vae_model.to(device)
    vae_model.eval()

    # æå–ç¼–ç å™¨
    encoder = vae_model.encoder
    encoder.requires_grad_(False)

    print("âœ“ Encoder loaded")
    return encoder


def _load_diffusion_model(checkpoint_path, config, device):
    """åŠ è½½åˆ†æ•°ç½‘ç»œæ¨¡å‹"""
    print(f"Loading diffusion model from: {checkpoint_path}")

    # è¿™é‡Œéœ€è¦åŠ è½½åˆ†æ•°ç½‘ç»œï¼Œæš‚æ—¶è¿”å›Noneè¡¨ç¤ºæœªå®ç°
    # TODO: å®ç°åˆ†æ•°ç½‘ç»œçš„åŠ è½½é€»è¾‘
    print("âš ï¸ åˆ†æ•°ç½‘ç»œåŠ è½½åŠŸèƒ½å¾…å®ç°")
    return None


def _get_embeddings(encoder, x, adj, device):
    """è·å–å›¾åµŒå…¥"""
    mask = node_flags(adj).unsqueeze(-1)

    with torch.no_grad():
        # ä½¿ç”¨ç¼–ç å™¨æå–ç‰¹å¾
        posterior = encoder(x, adj, mask)

        # å¤„ç†åˆ†å¸ƒè¾“å‡º - è·å–å‡å€¼æˆ–æ¨¡å¼
        if hasattr(posterior, "mode"):
            z = posterior.mode()
        elif hasattr(posterior, "mean"):
            z = posterior.mean
        else:
            z = posterior

        # æ£€æŸ¥æ˜¯å¦åœ¨åŒæ›²æµå½¢ä¸Š
        if hasattr(encoder, "manifold") and encoder.manifold is not None:
            # åŒæ›²ç©ºé—´ï¼šä½¿ç”¨æµå½¢ä¸Šçš„å¹³å‡æ± åŒ–
            manifold = encoder.manifold

            # åœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡Œmasked pooling
            mask_expanded = mask.expand_as(z)

            # å°†æ— æ•ˆèŠ‚ç‚¹æŠ•å½±åˆ°åŸç‚¹ï¼ˆåœ¨åŒæ›²ç©ºé—´ä¸­ï¼‰
            z_masked = z * mask_expanded

            # è®¡ç®—æœ‰æ•ˆèŠ‚ç‚¹æ•°
            num_valid_nodes = mask.sum(dim=1, keepdim=True).float()
            num_valid_nodes = torch.clamp(num_valid_nodes, min=1.0)

            # åœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡Œå¹³å‡ï¼ˆä½¿ç”¨Einsteinä¸­ç‚¹ï¼‰
            # ç®€åŒ–ç‰ˆæœ¬ï¼šå…ˆè½¬æ¢åˆ°åˆ‡ç©ºé—´ï¼Œå¹³å‡ï¼Œå†æŠ•å½±å›æµå½¢
            z_tangent = manifold.logmap0(z_masked)

            # åœ¨åˆ‡ç©ºé—´ä¸­å¹³å‡
            graph_embeddings = z_tangent.sum(dim=1) / num_valid_nodes

            # æŠ•å½±å›æµå½¢
            graph_embeddings = manifold.expmap0(graph_embeddings)

        else:
            # æ¬§å‡ é‡Œå¾—ç©ºé—´ï¼šæ ‡å‡†å¹³å‡æ± åŒ–
            mask_expanded = mask.expand_as(z)
            z_masked = z * mask_expanded

            # è®¡ç®—æœ‰æ•ˆèŠ‚ç‚¹æ•°
            num_valid_nodes = mask.sum(dim=1, keepdim=True).float()
            num_valid_nodes = torch.clamp(num_valid_nodes, min=1.0)

            # å¹³å‡æ± åŒ–
            graph_embeddings = z_masked.sum(dim=1) / num_valid_nodes

    return graph_embeddings


def _augment_data(data, diffusion_model, k_augment=5):
    """ä½¿ç”¨åˆ†æ•°ç½‘ç»œè¿›è¡Œæ•°æ®å¢å¼º"""
    # TODO: å®ç°æ•°æ®å¢å¼ºé€»è¾‘
    return data


def _train_classifier_on_task(
    task, encoder, diffusion_model, config, device, use_augmentation=False
):
    """åœ¨å•ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒåˆ†ç±»å™¨"""
    # è·å–æ•°æ®
    support_x = task["support_set"]["x"].to(device)
    support_adj = task["support_set"]["adj"].to(device)
    support_labels = task["support_set"]["label"].to(device)

    query_x = task["query_set"]["x"].to(device)
    query_adj = task["query_set"]["adj"].to(device)
    query_labels = task["query_set"]["label"].to(device)

    # æ•°æ®å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_augmentation and diffusion_model is not None:
        k_augment = getattr(config.fsl_task, "k_augment", 5)
        # å¢å¼ºæ”¯æŒé›†æ•°æ®
        support_data = {"x": support_x, "adj": support_adj, "labels": support_labels}
        augmented_support = _augment_data(support_data, diffusion_model, k_augment)

        # ä½¿ç”¨å¢å¼ºåçš„æ•°æ®
        support_x = augmented_support["x"]
        support_adj = augmented_support["adj"]
        support_labels = augmented_support["labels"]

    # è·å–åµŒå…¥
    support_emb = _get_embeddings(encoder, support_x, support_adj, device)
    query_emb = _get_embeddings(encoder, query_x, query_adj, device)

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„ - å…³é”®ä¿®å¤ï¼šä½¿ç”¨è¿ç»­çš„æ ‡ç­¾æ˜ å°„
    unique_support_labels = torch.unique(support_labels)
    unique_query_labels = torch.unique(query_labels)

    # ç¡®ä¿æŸ¥è¯¢æ ‡ç­¾éƒ½åœ¨æ”¯æŒæ ‡ç­¾ä¸­
    all_labels = torch.unique(torch.cat([support_labels, query_labels]))
    # åˆ›å»ºä»åŸå§‹æ ‡ç­¾åˆ°è¿ç»­æ ‡ç­¾çš„æ˜ å°„
    label_map = {label.item(): idx for idx, label in enumerate(all_labels)}

    # æ˜ å°„æ ‡ç­¾
    mapped_support_labels = torch.tensor(
        [label_map[label.item()] for label in support_labels], device=device, dtype=torch.long
    )
    mapped_query_labels = torch.tensor(
        [label_map[label.item()] for label in query_labels], device=device, dtype=torch.long
    )

    # åˆ›å»ºåˆ†ç±»å™¨
    num_classes = len(all_labels)
    # ä¿®å¤ï¼šsupport_embçš„å½¢çŠ¶å¯èƒ½æ˜¯[batch_size, num_nodes, embedding_dim]ï¼Œéœ€è¦è·å–æ­£ç¡®çš„ç»´åº¦
    if support_emb.dim() == 3:
        # å¦‚æœæ˜¯3ç»´ï¼Œè¯´æ˜æ˜¯[batch_size, num_nodes, embedding_dim]ï¼Œéœ€è¦æ± åŒ–
        embedding_dim = support_emb.size(-1)

        # æŒ‰ç…§GraphVAEä¸­çš„å¤„ç†æ–¹å¼ï¼šmean + max poolingç„¶åè¿æ¥
        support_mean = support_emb.mean(dim=1)  # [batch_size, embedding_dim]
        support_max = support_emb.max(dim=1).values  # [batch_size, embedding_dim]
        support_emb_concat = torch.cat(
            [support_mean, support_max], dim=-1
        )  # [batch_size, embedding_dim*2]

        query_mean = query_emb.mean(dim=1)
        query_max = query_emb.max(dim=1).values
        query_emb_concat = torch.cat([query_mean, query_max], dim=-1)

        model_dim = embedding_dim  # ClassifieræœŸæœ›çš„model_dimæ˜¯å•ä¸ªåµŒå…¥ç»´åº¦
    else:
        # å¦‚æœæ˜¯2ç»´ï¼Œè¯´æ˜å·²ç»æ˜¯[batch_size, embedding_dim]
        # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬å‡è®¾å·²ç»æ˜¯å¤„ç†è¿‡çš„ç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨
        model_dim = support_emb.size(1) // 2  # å‡è®¾å·²ç»æ˜¯è¿æ¥åçš„ç‰¹å¾
        support_emb_concat = support_emb
        query_emb_concat = query_emb

    classifier = Classifier(
        model_dim=model_dim,
        num_classes=num_classes,
        classifier_dropout=0.2,
        classifier_bias=True,
        manifold=None,  # ä½¿ç”¨æ¬§å‡ é‡Œå¾—ç©ºé—´
    ).to(device)

    # è®­ç»ƒåˆ†ç±»å™¨ - æ”¹è¿›è®­ç»ƒè¿‡ç¨‹
    optimizer = optim.Adam(classifier.parameters(), lr=0.01, weight_decay=0.0001)
    criterion = nn.CrossEntropyLoss()

    classifier.train()

    # å¢åŠ è®­ç»ƒè½®æ•°å¹¶æ·»åŠ éªŒè¯
    num_epochs = 100
    best_val_loss = float("inf")
    patience = 10
    no_improve_count = 0

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        logits = classifier(support_emb_concat)
        loss = criterion(logits, mapped_support_labels)
        loss.backward()
        optimizer.step()

        # ç®€å•çš„éªŒè¯ - åœ¨æ”¯æŒé›†ä¸Šæµ‹è¯•
        if epoch % 10 == 0:
            classifier.eval()
            with torch.no_grad():
                val_logits = classifier(support_emb_concat)
                val_loss = criterion(val_logits, mapped_support_labels)
                val_preds = torch.argmax(val_logits, dim=1)
                val_acc = (val_preds == mapped_support_labels).float().mean()

                print(
                    f"  Epoch {epoch}: loss={loss.item():.4f}, val_loss={val_loss.item():.4f}, val_acc={val_acc.item():.4f}"
                )

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    no_improve_count = 0
                else:
                    no_improve_count += 1

                if no_improve_count >= patience:
                    print(f"  Early stopping at epoch {epoch}")
                    break

            classifier.train()

    # è¯„ä¼°
    classifier.eval()
    with torch.no_grad():
        query_logits = classifier(query_emb_concat)
        query_preds = torch.argmax(query_logits, dim=1)

        # è®¡ç®—æŒ‡æ ‡
        accuracy = (query_preds == mapped_query_labels).float().mean().item()

        y_true = mapped_query_labels.cpu().numpy()
        y_pred = query_preds.cpu().numpy()

        f1 = f1_score(y_true, y_pred, average="macro", zero_division=0)
        precision = precision_score(y_true, y_pred, average="macro", zero_division=0)
        recall = recall_score(y_true, y_pred, average="macro", zero_division=0)

    return {
        "accuracy": accuracy,
        "f1": f1,
        "precision": precision,
        "recall": recall,
    }


def _run_evaluation(dataset, encoder, diffusion_model, config, device, use_augmentation=False):
    """è¿è¡Œå…ƒæµ‹è¯•è¯„ä¼°"""
    print(f"ğŸš€ Starting meta-test... (å¢å¼ºæ¨¡å¼: {use_augmentation})")

    results = []
    num_tasks = getattr(config.fsl_task, "num_test_tasks", 100)

    progress_bar = tqdm(range(num_tasks), desc="Meta-test")

    for task_idx in progress_bar:
        try:
            # é‡‡æ ·ä»»åŠ¡ - ä¿®å¤å‚æ•°
            task = dataset.sample_one_task(
                is_train=False,  # ä»æµ‹è¯•é›†é‡‡æ ·
                N_way=config.fsl_task.N_way,
                K_shot=config.fsl_task.K_shot,
                R_query=config.fsl_task.R_query,
            )

            if task is None:
                continue

            # è®­ç»ƒå¹¶è¯„ä¼°
            result = _train_classifier_on_task(
                task, encoder, diffusion_model, config, device, use_augmentation
            )
            results.append(result)

            # æ›´æ–°è¿›åº¦æ¡
            if results:
                avg_acc = np.mean([r["accuracy"] for r in results])
                progress_bar.set_postfix({"Avg Acc": f"{avg_acc:.4f}"})

            # è®°å½•ä¸­é—´ç»“æœ
            if (task_idx + 1) % 10 == 0:
                avg_acc = np.mean([r["accuracy"] for r in results])
                avg_f1 = np.mean([r["f1"] for r in results])
                wandb.log(
                    {
                        "avg_accuracy": avg_acc,
                        "avg_f1": avg_f1,
                        "completed_tasks": task_idx + 1,
                    }
                )

        except Exception as e:
            continue

    # è®¡ç®—æœ€ç»ˆç»“æœ
    if results:
        accuracies = [r["accuracy"] for r in results]
        f1_scores = [r["f1"] for r in results]

        final_acc = np.mean(accuracies)
        final_f1 = np.mean(f1_scores)
        std_acc = np.std(accuracies)
        std_f1 = np.std(f1_scores)

        # 95%ç½®ä¿¡åŒºé—´
        margin_acc = 1.96 * std_acc / np.sqrt(len(accuracies))
        margin_f1 = 1.96 * std_f1 / np.sqrt(len(f1_scores))

        # è®°å½•æœ€ç»ˆç»“æœ
        wandb.log(
            {
                "final_accuracy": final_acc,
                "final_f1": final_f1,
                "accuracy_std": std_acc,
                "f1_std": std_f1,
                "accuracy_margin": margin_acc,
                "f1_margin": margin_f1,
                "num_tasks": len(results),
                "use_augmentation": use_augmentation,
            }
        )

        # æ‰“å°ç»“æœ
        aug_status = "å¢å¼ºæ¨¡å¼" if use_augmentation else "åŸºç¡€æ¨¡å¼"
        print("\n" + "=" * 50)
        print(f"ğŸ“Š FINAL RESULTS ({aug_status})")
        print("=" * 50)
        print(f"Number of tasks: {len(results)}")
        print(f"Accuracy: {final_acc:.4f} Â± {margin_acc:.4f}")
        print(f"F1 Score: {final_f1:.4f} Â± {margin_f1:.4f}")
        print("=" * 50)

        return {
            "accuracy": final_acc,
            "f1": final_f1,
            "num_tasks": len(results),
            "use_augmentation": use_augmentation,
        }
    else:
        print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•ä»»åŠ¡!")
        return {
            "accuracy": 0.0,
            "f1": 0.0,
            "num_tasks": 0,
            "use_augmentation": use_augmentation,
        }


def existing_task_test(encoder, graph_embedding_net, task, config, device, use_augmentation=False):
    """
    å¯¹å•ä¸ªä»»åŠ¡è¿›è¡Œæµ‹è¯•ï¼Œä½¿ç”¨ç°æœ‰çš„æ–¹æ³•
    """
    try:
        # æå–ä»»åŠ¡æ•°æ®
        support_x = task["support_set"]["x"].to(device)
        support_adj = task["support_set"]["adj"].to(device)
        support_labels = task["support_set"]["y"].to(device)
        query_x = task["query_set"]["x"].to(device)
        query_adj = task["query_set"]["adj"].to(device)
        query_labels = task["query_set"]["y"].to(device)

        N_way = config.fsl_task.N_way

        with torch.no_grad():
            # ç¼–ç å›¾å½¢
            support_node_emb = encoder(support_x, support_adj)
            query_node_emb = encoder(query_x, query_adj)

            # è·å–å›¾çº§åˆ«åµŒå…¥
            support_graph_emb = graph_embedding_net(support_node_emb, support_adj)
            query_graph_emb = graph_embedding_net(query_node_emb, query_adj)

            support_emb = support_graph_emb
            query_emb = query_graph_emb

            # æ ‡ç­¾ä¿¡æ¯
            unique_labels = torch.unique(torch.cat([support_labels, query_labels]))

            # æ ‡ç­¾æ˜ å°„ï¼šå°†åŸå§‹æ ‡ç­¾æ˜ å°„åˆ°0, 1, 2, ..., N_way-1
            label_to_new = {label.item(): i for i, label in enumerate(unique_labels)}

            mapped_support_labels = torch.tensor(
                [label_to_new[label.item()] for label in support_labels], device=device
            )
            mapped_query_labels = torch.tensor(
                [label_to_new[label.item()] for label in query_labels], device=device
            )

            # è¿æ¥å›¾åµŒå…¥å’ŒèŠ‚ç‚¹åµŒå…¥ç‰¹å¾
            support_emb_concat = torch.cat([support_emb, support_node_emb.mean(dim=1)], dim=-1)
            query_emb_concat = torch.cat([query_emb, query_node_emb.mean(dim=1)], dim=-1)

            # è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨
            classifier = nn.Linear(support_emb_concat.size(-1), N_way).to(device)
            optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01)
            criterion = nn.CrossEntropyLoss()

            # è®­ç»ƒ100ä¸ªepoch
            for epoch in range(100):
                optimizer.zero_grad()
                logits = classifier(support_emb_concat)
                loss = criterion(logits, mapped_support_labels)
                loss.backward()
                optimizer.step()

            # æµ‹è¯•
            with torch.no_grad():
                query_logits = classifier(query_emb_concat)
                query_preds = torch.argmax(query_logits, dim=1)

                # è®¡ç®—å‡†ç¡®ç‡
                correct = (query_preds == mapped_query_labels).sum().item()
                total = mapped_query_labels.size(0)
                accuracy = correct / total

                return {"accuracy": accuracy}

    except Exception as e:
        print(f"Task test error: {e}")
        return {"accuracy": 0.0}
